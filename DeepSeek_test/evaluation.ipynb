{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9606e0f3",
   "metadata": {},
   "source": [
    "# Local Ollama Chatbot (Qwen)\n",
    "\n",
    "This notebook runs a minimal chatbot against a **local** Ollama model via its OpenAI-compatible API.\n",
    "\n",
    "Prereq:\n",
    "- Install Ollama and run it\n",
    "- Ensure a model is pulled (example: `ollama pull qwen3:8b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fe6dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_OPENAI_BASE_URL: http://localhost:11434/v1\n",
      "MODEL_NAME: qwen3:8b\n"
     ]
    }
   ],
   "source": [
    "# Local backend setup (Ollama)\n",
    "import os\n",
    "\n",
<<<<<<< HEAD
    "# Ollama OpenAI-compatible base URL\n",
    "os.environ.setdefault(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\")\n",
    "\n",
    "# Model to use (text-only Qwen is recommended for this notebook)\n",
    "MODEL_NAME = os.getenv(\"LOCAL_MODEL_NAME\", \"qwen3:8b\")\n",
    "\n",
    "# Optional: some gateways require a key; Ollama accepts any value.\n",
    "# os.environ.setdefault(\"LOCAL_OPENAI_API_KEY\", \"ollama\")\n",
    "\n",
    "print(\"LOCAL_OPENAI_BASE_URL:\", os.getenv(\"LOCAL_OPENAI_BASE_URL\"))\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)"
||||||| 73355e7
    "# Recommended: set HF_TOKEN in your OS environment and restart the kernel.\n",
    "# Example (PowerShell): setx HF_TOKEN \"hf_...\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
=======
    "# Recommended: set HF_TOKEN in your OS environment and restart the kernel.\n",
    "# Example (PowerShell): setx HF_TOKEN \"hf_...\"\n",
    "os.environ[\"HF_TOKEN\"] = \"----\""
>>>>>>> bf6df2019af22d37103bd857f3d90c6f1e0aa079
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b89c805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:11434/v1/models -> 200\n",
      "{\"object\":\"list\",\"data\":[{\"id\":\"qwen3:8b\",\"object\":\"model\",\"created\":1767692280,\"owned_by\":\"library\"},{\"id\":\"deepseek-r1:8b\",\"object\":\"model\",\"created\":1767688957,\"owned_by\":\"library\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connectivity check: local Ollama OpenAI-compatible API\n",
    "import os, requests\n",
    "\n",
    "base = os.getenv(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\").rstrip(\"/\")\n",
    "url = f\"{base}/models\"\n",
    "r = requests.get(url, timeout=10)\n",
    "print(\"GET\", url, \"->\", r.status_code)\n",
    "print((r.text or \"\")[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11687c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST http://localhost:11434/v1/chat/completions -> 200\n",
      "{\"id\":\"chatcmpl-158\",\"object\":\"chat.completion\",\"created\":1767692378,\"model\":\"qwen3:8b\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\",\"reasoning\":\"Okay, the user wants me to say hello in one sentence. Let me think about how to approach this. First,\n"
     ]
    }
   ],
   "source": [
    "# Test local chat completions (optional)\n",
    "import os, requests, json\n",
    "\n",
    "base = os.getenv(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\").rstrip(\"/\")\n",
    "url = f\"{base}/chat/completions\"\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n",
    "    \"max_tokens\": 32,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "print(\"POST\", url, \"->\", resp.status_code)\n",
    "print((resp.text or \"\")[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "520e5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "LOCAL_OPENAI_BASE_URL: http://localhost:11434/v1\n",
      "MODEL_NAME: qwen3:8b\n",
      "Try: bot.reply('Hello!')\n"
     ]
    }
   ],
   "source": [
    "import importlib, os\n",
    "\n",
    "import deepseek_chat\n",
    "importlib.reload(deepseek_chat)\n",
    "\n",
    "from deepseek_chat import DeepSeekChatbot\n",
    "\n",
    "bot = DeepSeekChatbot(model=MODEL_NAME)\n",
    "print(\"Ready.\")\n",
    "print(\"LOCAL_OPENAI_BASE_URL:\", os.getenv(\"LOCAL_OPENAI_BASE_URL\"))\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"Try: bot.reply('Hello!')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20048336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# Smoke test\n",
    "print(bot.reply(\"Say hello in one sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d42c5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PDF attachment (Hybrid RAG: BM25 + Embeddings + Multi-step) ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import math\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Install / import PDF extraction\n",
    "try:\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pypdf\"])\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "\n",
    "# Widgets (optional UI)\n",
    "try:\n",
    "    import ipywidgets as widgets  # type: ignore\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    display = None\n",
    "\n",
    "# Hybrid retrieval deps\n",
    "import numpy as np\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rank-bm25\"])\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "except Exception:\n",
    "    SentenceTransformer = None  # type: ignore\n",
    "\n",
    "\n",
    "_WORD_RE = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "_STOPWORDS = {\n",
    "    # EN\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"is\",\"are\",\"was\",\"were\",\"be\",\"as\",\"at\",\"by\",\"it\",\"this\",\"that\",\"from\",\"into\",\"over\",\"under\",\"not\",\n",
    "    # FR\n",
    "    \"le\",\"la\",\"les\",\"un\",\"une\",\"des\",\"et\",\"ou\",\"de\",\"du\",\"dans\",\"sur\",\"pour\",\"avec\",\"est\",\"sont\",\"été\",\"être\",\"ce\",\"cet\",\"cette\",\"ces\",\"pas\",\"plus\",\n",
    "}\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    words = [w.lower() for w in _WORD_RE.findall(text)]\n",
    "    return [w for w in words if len(w) >= 2 and w not in _STOPWORDS]\n",
    "\n",
    "\n",
    "def _pdf_bytes_to_text(data: bytes) -> str:\n",
    "    reader = PdfReader(io.BytesIO(data))\n",
    "    parts: List[str] = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            parts.append(page.extract_text() or \"\")\n",
    "        except Exception:\n",
    "            parts.append(\"\")\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "\n",
    "def load_pdf_from_path(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return _pdf_bytes_to_text(f.read())\n",
    "\n",
    "\n",
    "_COLREG_HEAD_RE = re.compile(\n",
    "    r\"(?im)^(rule\\s+\\d+\\b.*|annex\\s+[ivx]+\\b.*|appendix\\b.*)$\"\n",
    " )\n",
    "\n",
    "\n",
    "def _normalize_ws(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def chunk_colreg(text: str, *, max_chars: int = 1600, overlap: int = 250) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Chunk COLREG-like docs by headings (Rule/Annex), then slice each section.\"\"\"\n",
    "    raw = text.replace(\"\\r\\n\", \"\\n\")\n",
    "    lines = [ln.strip() for ln in raw.split(\"\\n\")]\n",
    "    headings: List[Tuple[int, str]] = []\n",
    "    for i, ln in enumerate(lines):\n",
    "        if not ln:\n",
    "            continue\n",
    "        m = _COLREG_HEAD_RE.match(ln)\n",
    "        if m:\n",
    "            headings.append((i, m.group(1).strip()))\n",
    "    if not headings:\n",
    "        # Fallback: no headings detected -> single section\n",
    "        joined = _normalize_ws(raw)\n",
    "        return [(\"Document\", ch) for ch in _slice_text(joined, max_chars=max_chars, overlap=overlap)]\n",
    "\n",
    "    # Ensure start at first heading\n",
    "    chunks: List[Tuple[str, str]] = []\n",
    "    for idx, (start_i, title) in enumerate(headings):\n",
    "        end_i = headings[idx + 1][0] if idx + 1 < len(headings) else len(lines)\n",
    "        section_text = _normalize_ws(\" \".join([ln for ln in lines[start_i:end_i] if ln]))\n",
    "        if not section_text:\n",
    "            continue\n",
    "        for ch in _slice_text(section_text, max_chars=max_chars, overlap=overlap):\n",
    "            chunks.append((title, ch))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _slice_text(text: str, *, max_chars: int, overlap: int) -> List[str]:\n",
    "    text = _normalize_ws(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + max_chars)\n",
    "        out.append(text[start:end])\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _minmax(scores: np.ndarray) -> np.ndarray:\n",
    "    if scores.size == 0:\n",
    "        return scores\n",
    "    mn = float(scores.min())\n",
    "    mx = float(scores.max())\n",
    "    if mx - mn < 1e-9:\n",
    "        return np.zeros_like(scores, dtype=float)\n",
    "    return (scores - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    idx: int\n",
    "    section: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttachedPDF:\n",
    "    name: str\n",
    "    text: str\n",
    "    chunks: List[Chunk]\n",
    "    bm25: BM25Okapi\n",
    "    bm25_tokens: List[List[str]]\n",
    "    embedder_name: str\n",
    "    embeddings: Optional[np.ndarray]  # shape: (n, d) or None\n",
    "\n",
    "\n",
    "ATTACHED_PDF: AttachedPDF | None = None\n",
    "RETRIEVE_K = 6\n",
    "BM25_WEIGHT = 0.55\n",
    "SEM_WEIGHT = 0.45\n",
    "EMBEDDER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "_EMBEDDER: Optional[SentenceTransformer] = None  # type: ignore\n",
    "\n",
    "\n",
    "def _get_embedder() -> Optional[\"SentenceTransformer\"]:\n",
    "    global _EMBEDDER\n",
    "    if SentenceTransformer is None:\n",
    "        return None\n",
    "    if _EMBEDDER is None:\n",
    "        _EMBEDDER = SentenceTransformer(EMBEDDER_NAME)\n",
    "    return _EMBEDDER\n",
    "\n",
    "\n",
    "def attach_pdf_text(name: str, text: str) -> None:\n",
    "    \"\"\"Attach PDF text and build hybrid indexes (BM25 + embeddings).\"\"\"\n",
    "    global ATTACHED_PDF\n",
    "    pairs = chunk_colreg(text)\n",
    "    chunks = [Chunk(idx=i, section=sec, text=ch) for i, (sec, ch) in enumerate(pairs)]\n",
    "    bm25_tokens = [_tokenize(c.text) for c in chunks]\n",
    "    bm25 = BM25Okapi(bm25_tokens)\n",
    "\n",
    "    embedder = _get_embedder()\n",
    "    embeddings: Optional[np.ndarray] = None\n",
    "    if embedder is not None:\n",
    "        # Encode sections+text (helps retrieval by rule name)\n",
    "        docs = [f\"{c.section}. {c.text}\" for c in chunks]\n",
    "        emb = embedder.encode(docs, normalize_embeddings=True, show_progress_bar=False)\n",
    "        embeddings = np.asarray(emb, dtype=np.float32)\n",
    "\n",
    "    ATTACHED_PDF = AttachedPDF(\n",
    "        name=name,\n",
    "        text=text,\n",
    "        chunks=chunks,\n",
    "        bm25=bm25,\n",
    "        bm25_tokens=bm25_tokens,\n",
    "        embedder_name=EMBEDDER_NAME,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f\"Attached PDF: {name} ({len(text):,} chars, {len(chunks)} chunks)\")\n",
    "    if embeddings is None:\n",
    "        print(\"(Embeddings disabled: sentence-transformers not available)\")\n",
    "\n",
    "\n",
    "def clear_pdf() -> None:\n",
    "    global ATTACHED_PDF\n",
    "    ATTACHED_PDF = None\n",
    "    print(\"PDF cleared\")\n",
    "\n",
    "\n",
    "def _hybrid_scores(question: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert ATTACHED_PDF is not None\n",
    "    q_tokens = _tokenize(question)\n",
    "    bm25_scores = np.asarray(ATTACHED_PDF.bm25.get_scores(q_tokens), dtype=float)\n",
    "    bm25_norm = _minmax(bm25_scores)\n",
    "\n",
    "    sem_norm = np.zeros_like(bm25_norm, dtype=float)\n",
    "    if ATTACHED_PDF.embeddings is not None:\n",
    "        embedder = _get_embedder()\n",
    "        if embedder is not None:\n",
    "            q_emb = embedder.encode([question], normalize_embeddings=True, show_progress_bar=False)\n",
    "            q_emb = np.asarray(q_emb[0], dtype=np.float32)\n",
    "            # cosine because embeddings are normalized => dot product = cosine\n",
    "            sem = np.dot(ATTACHED_PDF.embeddings, q_emb).astype(float)\n",
    "            sem_norm = _minmax(sem)\n",
    "\n",
    "    hybrid = BM25_WEIGHT * bm25_norm + SEM_WEIGHT * sem_norm\n",
    "    return hybrid, bm25_norm, sem_norm\n",
    "\n",
    "\n",
    "def _diverse_topk(indices: List[int], *, k: int) -> List[int]:\n",
    "    \"\"\"Keep a diverse set of chunks by section to avoid near-duplicates.\"\"\"\n",
    "    assert ATTACHED_PDF is not None\n",
    "    picked: List[int] = []\n",
    "    seen_sections: Dict[str, int] = {}\n",
    "    for idx in indices:\n",
    "        sec = ATTACHED_PDF.chunks[idx].section\n",
    "        # allow up to 2 chunks per section in the final context\n",
    "        if seen_sections.get(sec, 0) >= 2:\n",
    "            continue\n",
    "        picked.append(idx)\n",
    "        seen_sections[sec] = seen_sections.get(sec, 0) + 1\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "    return picked\n",
    "\n",
    "\n",
    "def retrieve_context(question: str, *, k: int = 6, pool: int = 20) -> List[int]:\n",
    "    \"\"\"Hybrid retrieval with diversity: returns chunk indices.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        return []\n",
    "    hybrid, _, _ = _hybrid_scores(question)\n",
    "    ranked = np.argsort(-hybrid)[: max(pool, k)].tolist()\n",
    "    ranked = [i for i in ranked if hybrid[i] > 0]\n",
    "    return _diverse_topk(ranked, k=k)\n",
    "\n",
    "\n",
    "def _build_context(indices: List[int]) -> str:\n",
    "    assert ATTACHED_PDF is not None\n",
    "    if not indices:\n",
    "        return \"\"\n",
    "    blocks: List[str] = []\n",
    "    for idx in indices:\n",
    "        c = ATTACHED_PDF.chunks[idx]\n",
    "        blocks.append(f\"[Chunk {idx+1} | {c.section}] {c.text}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "\n",
    "def _generate_search_queries(question: str) -> List[str]:\n",
    "    \"\"\"Ask the LLM for 3 short retrieval queries (multi-step RAG).\"\"\"\n",
    "    prompt = (\n",
    "        \"Generate 3 short search queries (keywords + any relevant Rule/Annex numbers) \"\n",
    "        \"to find the answer in the COLREG document. Output only the queries, one per line.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "    raw = bot.reply(prompt, max_tokens=128, temperature=0.2)\n",
    "    lines = [ln.strip(\" -\\t\") for ln in raw.splitlines() if ln.strip()]\n",
    "    # keep at most 3\n",
    "    return lines[:3] if lines else [question]\n",
    "\n",
    "\n",
    "def retrieve_context_multistep(question: str, *, k: int = 6) -> List[int]:\n",
    "    \"\"\"Two-pass retrieval: initial -> query expansion -> merged retrieval.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        return []\n",
    "    # Pass 1\n",
    "    init = retrieve_context(question, k=max(4, k), pool=30)\n",
    "    # Pass 2: query expansion\n",
    "    queries = _generate_search_queries(question)\n",
    "    merged_scores = np.zeros(len(ATTACHED_PDF.chunks), dtype=float)\n",
    "    # seed with initial question\n",
    "    for q in [question] + queries:\n",
    "        hybrid, _, _ = _hybrid_scores(q)\n",
    "        merged_scores = np.maximum(merged_scores, hybrid)\n",
    "    ranked = np.argsort(-merged_scores)[: 40].tolist()\n",
    "    ranked = [i for i in ranked if merged_scores[i] > 0]\n",
    "    # ensure we keep some of the pass-1 results\n",
    "    for idx in init:\n",
    "        if idx in ranked:\n",
    "            ranked.remove(idx)\n",
    "        ranked.insert(0, idx)\n",
    "    # Diverse final\n",
    "    return _diverse_topk(ranked, k=k)\n",
    "\n",
    "\n",
    "def ask_on_pdf(question: str, *, k: int | None = None, multistep: bool = True) -> str:\n",
    "    if not ATTACHED_PDF:\n",
    "        return bot.reply(question)\n",
    "    k = RETRIEVE_K if k is None else k\n",
    "    indices = retrieve_context_multistep(question, k=k) if multistep else retrieve_context(question, k=k)\n",
    "    context = _build_context(indices)\n",
    "    prompt = (\n",
    "        \"You are evaluating a model. Answer the user's question using ONLY the provided document excerpts. \"\n",
    "        \"Cite the chunks you used like (Chunk 12). If the excerpts do not contain the answer, say you don't know.\\n\\n\"\n",
    "        f\"Document: {ATTACHED_PDF.name}\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\"\n",
    "    )\n",
    "    return bot.reply(prompt)\n",
    "\n",
    "\n",
    "def _extract_first_upload(value) -> tuple[str, bytes] | None:\n",
    "    \"\"\"Return (filename, content_bytes) for the first uploaded file.\"\"\"\n",
    "    if not value:\n",
    "        return None\n",
    "    if isinstance(value, dict):\n",
    "        first_key = next(iter(value.keys()))\n",
    "        first = value[first_key]\n",
    "        if isinstance(first, dict) and \"content\" in first:\n",
    "            name = first.get(\"name\") or first_key\n",
    "            return name, first[\"content\"]\n",
    "        if isinstance(first, (bytes, bytearray)):\n",
    "            return first_key, bytes(first)\n",
    "    if isinstance(value, (tuple, list)):\n",
    "        first = value[0]\n",
    "        if isinstance(first, dict) and \"content\" in first:\n",
    "            name = first.get(\"name\") or \"uploaded.pdf\"\n",
    "            return name, first[\"content\"]\n",
    "        name = getattr(first, \"name\", \"uploaded.pdf\")\n",
    "        content = getattr(first, \"content\", None)\n",
    "        if content is None:\n",
    "            content = getattr(first, \"data\", None)\n",
    "        if content is None and isinstance(first, (bytes, bytearray)):\n",
    "            content = bytes(first)\n",
    "        if isinstance(content, (bytes, bytearray)):\n",
    "            return name, bytes(content)\n",
    "    name = getattr(value, \"name\", \"uploaded.pdf\")\n",
    "    content = getattr(value, \"content\", None)\n",
    "    if isinstance(content, (bytes, bytearray)):\n",
    "        return name, bytes(content)\n",
    "    return None\n",
    "\n",
    "\n",
    "def show_pdf_uploader() -> None:\n",
    "    if widgets is None or display is None:\n",
    "        print(\"ipywidgets not available in this environment. Use attach_pdf_text(load_pdf_from_path(...))\")\n",
    "        return\n",
    "    uploader = widgets.FileUpload(accept=\".pdf\", multiple=False)\n",
    "    button = widgets.Button(description=\"Load PDF\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _on_click(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            item = _extract_first_upload(uploader.value)\n",
    "            if not item:\n",
    "                print(\"Select a PDF first\")\n",
    "                return\n",
    "            name, data = item\n",
    "            text = _pdf_bytes_to_text(data)\n",
    "            attach_pdf_text(name, text)\n",
    "\n",
    "    button.on_click(_on_click)\n",
    "    display(widgets.VBox([widgets.HTML(\"<b>Attach a PDF</b>\"), uploader, button, out]))\n",
    "\n",
    "\n",
    "# Run this to attach a PDF via UI (if supported):\n",
    "# show_pdf_uploader()\n",
    "\n",
    "# Or attach from a local path (Windows example):\n",
    "# attach_pdf_text(\"my.pdf\", load_pdf_from_path(r\"C:\\\\path\\\\to\\\\file.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f794fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bec729244634c1c81f3f97193ffd2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Attach a PDF</b>'), FileUpload(value=(), accept='.pdf', description='Upload'), B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pdf_uploader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d98cb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    \"\"\"Interactive loop.\n",
    "\n",
    "    Commands:\n",
    "      /reset       clear chat history\n",
    "      /exit        quit\n",
    "      /pdf         show PDF status\n",
    "      /clearpdf    detach current PDF\n",
    "      /k N         set retrieval chunks (default 4)\n",
    "    \"\"\"\n",
    "    global RETRIEVE_K\n",
    "    print(\"Type '/reset', '/exit'. Use '/pdf' after attaching a PDF.\")\n",
    "    while True:\n",
    "        user = input(\"You> \").strip()\n",
    "        if not user:\n",
    "            continue\n",
    "        low = user.lower()\n",
    "        if low in {\"/exit\", \"/quit\"}:\n",
    "            break\n",
    "        if low == \"/reset\":\n",
    "            bot.reset()\n",
    "            print(\"(history cleared)\")\n",
    "            continue\n",
    "        if low == \"/pdf\":\n",
    "            if ATTACHED_PDF:\n",
    "                print(f\"(pdf attached: {ATTACHED_PDF.name}, {len(ATTACHED_PDF.chunks)} chunks, k={RETRIEVE_K})\")\n",
    "            else:\n",
    "                print(\"(no pdf attached) -> run show_pdf_uploader() or attach_pdf_text(...)\")\n",
    "            continue\n",
    "        if low == \"/clearpdf\":\n",
    "            clear_pdf()\n",
    "            continue\n",
    "        if low.startswith(\"/k \"):\n",
    "            try:\n",
    "                RETRIEVE_K = max(1, int(low.split()[1]))\n",
    "                print(f\"(k set to {RETRIEVE_K})\")\n",
    "            except Exception:\n",
    "                print(\"Usage: /k 4\")\n",
    "            continue\n",
    "\n",
    "        answer = ask_on_pdf(user) if ATTACHED_PDF else bot.reply(user)\n",
    "        print(f\"Bot> {answer}\\n\")\n",
    "\n",
    "\n",
    "# Run interactive chat in the notebook output\n",
    "# chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146d9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type '/reset', '/exit'. Use '/pdf' after attaching a PDF.\n",
      "(pdf attached: COLREG-Consolidated-2018.pdf, 66 chunks, k=4)\n",
      "Bot> According to the provided document excerpts, \"turning to port\" is indicated by **two short blasts** on the whistle.\n",
      "\n",
      "**Source:** Chunk 32 states: \"two short blasts to mean 'I am altering my course to port'\".\n",
      "\n",
      "Bot> Based solely on the provided excerpts, the signal for \"turning to port\" is **two short blasts** on the whistle.\n",
      "\n",
      "**Source:** Chunk 32 explicitly states: \"two short blasts to mean 'I am altering my course to port'\".\n",
      "\n",
      "Bot> Based solely on the provided excerpts, the signal for \"overtake on starboard\" is **two prolonged blasts followed by one short blast** on the whistle.\n",
      "\n",
      "**Source:** Chunk 33 states: \"two prolonged blasts followed by one short blast to mean 'I intend to overtake you on your starboard side'\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81c21283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Set évaluation RAG.csv\n",
      "PDF: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\COLREG-Consolidated-2018.pdf\n",
      "Loaded 12 questions\n",
      "Attached PDF: COLREG-Consolidated-2018.pdf (104,429 chars, 95 chunks)\n"
     ]
    }
   ],
   "source": [
    "# --- Batch evaluation from CSV (RAG) ---\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_existing_path(*candidates: str) -> Path:\n",
    "    for c in candidates:\n",
    "        p = Path(c)\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    raise FileNotFoundError(f\"None of these paths exist: {candidates}\")\n",
    "\n",
    "\n",
    "def _read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        # Common fallbacks for Windows CSV exports\n",
    "        for enc in (\"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "\n",
    "# Notebook usually runs with CWD = DeepSeek_test/. These files are at the workspace root.\n",
    "CSV_PATH = _find_existing_path(\n",
    "    \"Set évaluation RAG.csv\",\n",
    "    str(Path(\"..\") / \"Set évaluation RAG.csv\"),\n",
    "    str(Path(\"..\") / \"..\" / \"Set évaluation RAG.csv\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Set évaluation RAG.csv\",\n",
    ")\n",
    "PDF_PATH = _find_existing_path(\n",
    "    \"COLREG-Consolidated-2018.pdf\",\n",
    "    str(Path(\"..\") / \"COLREG-Consolidated-2018.pdf\"),\n",
    "    str(Path(\"..\") / \"..\" / \"COLREG-Consolidated-2018.pdf\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\COLREG-Consolidated-2018.pdf\",\n",
    ")\n",
    "\n",
    "print(\"CSV:\", CSV_PATH)\n",
    "print(\"PDF:\", PDF_PATH)\n",
    "\n",
    "# Load CSV and keep only the Question column for now\n",
    "df_questions = _read_csv_robust(CSV_PATH)\n",
    "if \"Question\" not in df_questions.columns:\n",
    "    raise KeyError(f\"CSV must have a 'Question' column. Found: {list(df_questions.columns)}\")\n",
    "questions = df_questions[\"Question\"].dropna().astype(str).tolist()\n",
    "print(f\"Loaded {len(questions)} questions\")\n",
    "\n",
    "# Attach the PDF (only if not already attached or if you want to force reload)\n",
    "if ATTACHED_PDF is None or (ATTACHED_PDF.name != PDF_PATH.name):\n",
    "    attach_pdf_text(PDF_PATH.name, load_pdf_from_path(str(PDF_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84070ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10/12\n",
      "Done 12/12\n",
      "Saved: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\DeepSeek_test\\rag_eval_answers.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation: ask each question on the attached PDF\n",
    "MAX_QUESTIONS = None  # set to an int for a quick dry-run, e.g. 5\n",
    "\n",
    "questions_to_run = questions if not MAX_QUESTIONS else questions[:MAX_QUESTIONS]\n",
    "results = []\n",
    "total = len(questions_to_run)\n",
    "for i, q in enumerate(questions_to_run, start=1):\n",
    "    bot.reset()  # avoid history leakage across questions\n",
    "    ans = ask_on_pdf(\"I do not want search queries or keywords. I want you to write a complete, professional paragraph that answers the question using the technical details found in the chunks.\\n\" + q)\n",
    "    results.append({\"Question\": q, \"Answer\": ans})\n",
    "    if i % 10 == 0 or i == total:\n",
    "        print(f\"Done {i}/{total}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.head()\n",
    "\n",
    "# Save results next to the notebook\n",
    "OUT_PATH = Path(\"rag_eval_answers.csv\").resolve()\n",
    "df_results.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
