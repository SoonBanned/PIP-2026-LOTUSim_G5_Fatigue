{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3c444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1bd3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "CUDA: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c407afa818d461bb45a08979013efec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Charger LLaMA-3 8B sur GPU (CUDA) — 4-bit pour tenir en 8GB\n",
    "# =========================\n",
    "# Objectif perf: réduire la latence d'inférence. On utilise `model.generate()` (plus léger que pipeline).\n",
    "\n",
    "import time\n",
    "import gc\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "try:\n",
    "    # New API (PyTorch 2.9+)\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "except Exception:\n",
    "    # Backward-compatible API\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"CUDA non détecté. Installe un PyTorch CUDA et/ou vérifie le driver GPU.\"\n",
    "    )\n",
    "\n",
    "# Auth Hugging Face (évite les tokens en dur dans le notebook)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "else:\n",
    "    print(\"HF_TOKEN non défini. Si le modèle est 'gated', définis l'env var HF_TOKEN.\")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"Model:\", MODEL_NAME)\n",
    "print(\"CUDA:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Libère proprement l'ancien modèle/pipeline pour éviter OOM\n",
    "for _name in (\"gen\", \"model\", \"tokenizer\"):\n",
    "    if _name in globals():\n",
    "        try:\n",
    "            del globals()[_name]\n",
    "        except Exception:\n",
    "            pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    " )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map={\"\": 0},  # tout sur GPU:0\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def format_prompt(question: str, contexts: list[str]) -> str:\n",
    "    context_block = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    system = \"You are an assistant who only responds based on the provided CONTEXT.\"\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"CONTEXTE:\\n{context_block}\\n\\nQUESTION:\\n{question}\",\n",
    "            },\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "    return (\n",
    "        f\"SYSTEM:\\n{system}\\n\\n\"\n",
    "        f\"CONTEXTE:\\n{context_block}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"RÉPONSE:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def _generate(prompt: str, max_new_tokens: int) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_ids = out_ids[0, inputs[\"input_ids\"].shape[1] :]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def ask_pdf(\n",
    "    question: str,\n",
    "    k: int = 2,\n",
    "    max_new_tokens: int = 1024,\n",
    "    max_context_chars: int = 650,\n",
    "):\n",
    "    \"\"\"Fast path: small k + truncated contexts + short generation.\"\"\"\n",
    "    hits = retrieve(question, k=k)\n",
    "    contexts = [\n",
    "        f\"[chunk {idx} | score={score:.3f}]\\n{text[:max_context_chars]}\"\n",
    "        for idx, score, text in hits\n",
    "]\n",
    "    prompt = format_prompt(question, contexts)\n",
    "    answer = _generate(prompt, max_new_tokens=max_new_tokens)\n",
    "    return answer, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548c41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\COLREG-Consolidated-2018.pdf\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Select PDF (local)\n",
    "# =========================\n",
    "def _pick_pdf_with_tkinter() -> str | None:\n",
    "    \"\"\"Return a PDF path chosen via a file dialog, or None if unavailable.\"\"\"\n",
    "    try:\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    root = None\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes(\"-topmost\", True)\n",
    "        path = filedialog.askopenfilename(\n",
    "            title=\"Select a PDF\",\n",
    "            filetypes=[(\"PDF files\", \"*.pdf\"), (\"All files\", \"*\")],\n",
    "        )\n",
    "        return path or None\n",
    "    except Exception:\n",
    "        return None\n",
    "    finally:\n",
    "        if root is not None:\n",
    "            try:\n",
    "                root.destroy()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "def get_pdf_path(pdf_path: str | None = None) -> str:\n",
    "    \"\"\"Resolve the PDF path from (1) parameter, (2) CLI args, (3) env, (4) auto COLREG, (5) GUI picker, (6) input().\"\"\"\n",
    "    if pdf_path:\n",
    "        pdf = Path(pdf_path).expanduser()\n",
    "        if not pdf.is_file():\n",
    "            raise FileNotFoundError(f\"PDF not found: {pdf}\")\n",
    "        return str(pdf)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"RAG over a local PDF\", add_help=False)\n",
    "    parser.add_argument(\"--pdf\", dest=\"pdf_path\", help=\"Path to the PDF to ingest\")\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "    if args.pdf_path:\n",
    "        pdf = Path(args.pdf_path).expanduser()\n",
    "        if not pdf.is_file():\n",
    "            raise FileNotFoundError(f\"PDF not found: {pdf}\")\n",
    "        return str(pdf)\n",
    "\n",
    "    env_pdf = os.environ.get(\"PDF_PATH\")\n",
    "    if env_pdf:\n",
    "        pdf = Path(env_pdf).expanduser()\n",
    "        if not pdf.is_file():\n",
    "            raise FileNotFoundError(f\"PDF not found (PDF_PATH): {pdf}\")\n",
    "        return str(pdf)\n",
    "\n",
    "    # Auto-pick COLREG PDF if present (avoids slow dialogs)\n",
    "    candidates = [\n",
    "        Path(\"COLREG-Consolidated-2018.pdf\"),\n",
    "        Path(\"..\") / \"COLREG-Consolidated-2018.pdf\",\n",
    "        Path.cwd() / \"COLREG-Consolidated-2018.pdf\",\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        try:\n",
    "            if cand.is_file():\n",
    "                return str(cand.resolve())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    picked = _pick_pdf_with_tkinter()\n",
    "    if picked:\n",
    "        return picked\n",
    "\n",
    "    entered = input(\"Enter path to PDF: \").strip().strip('\"')\n",
    "    pdf = Path(entered).expanduser()\n",
    "    if not pdf.is_file():\n",
    "        raise FileNotFoundError(f\"PDF not found: {pdf}\")\n",
    "    return str(pdf)\n",
    "\n",
    "\n",
    "pdf_path = get_pdf_path()\n",
    "print(\"PDF:\", pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2527674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: 104192\n",
      "Nb chunks: 139\n",
      "Ex chunk:\n",
      " Convention on the International Regulations \n",
      "for Preventing Collisions at Sea, 1972 \n",
      "Consolidated edition, 2018 \n",
      " \n",
      "ARTICLE I \n",
      "General Obligations \n",
      "The Parties to the present Convention undertake to give effect to the Rules and other \n",
      "Annexes constituting the International Regulations for Preventing Collisions at Sea, 1972, \n",
      " \n",
      "ARTICLE II \n",
      "Signature, Ratification, Acceptance, Approval and Accession  ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 2) PDF -> texte\n",
    "# =========================\n",
    "def pdf_to_text(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    parts = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        txt = page.extract_text() or \"\"\n",
    "        # nettoyage léger\n",
    "        txt = txt.replace(\"\\u00ad\", \"\")  # soft hyphen\n",
    "        txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "        parts.append(txt.strip())\n",
    "    return \"\\n\\n\".join([p for p in parts if p])\n",
    "\n",
    "\n",
    "raw_text = pdf_to_text(pdf_path)\n",
    "print(\"Chars:\", len(raw_text))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Chunking (découpage)\n",
    "# =========================\n",
    "def chunk_text(text: str, chunk_size=900, overlap=150):\n",
    "    text = text.strip()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = chunk_text(raw_text, chunk_size=900, overlap=150)\n",
    "print(\"Nb chunks:\", len(chunks))\n",
    "print(\"Ex chunk:\\n\", chunks[0][:400], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0c9d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc83edd14fc425293a5e5999486f3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS GPU not available, using CPU index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 4) Embeddings + FAISS index (GPU si dispo)\n",
    "# =========================\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "# encode en float32 (FAISS)\n",
    "emb = embedder.encode(\n",
    "    chunks, batch_size=64, show_progress_bar=True, normalize_embeddings=True\n",
    ")\n",
    "emb = np.array(emb, dtype=\"float32\")\n",
    "\n",
    "dim = emb.shape[1]\n",
    "\n",
    "# Prefer FAISS GPU if available (keeps search fast and off CPU)\n",
    "use_faiss_gpu = False\n",
    "try:\n",
    "    n_gpus = faiss.get_num_gpus()\n",
    "    use_faiss_gpu = n_gpus > 0\n",
    "except Exception:\n",
    "    n_gpus = 0\n",
    "    use_faiss_gpu = False\n",
    "\n",
    "if use_faiss_gpu:\n",
    "    print(f\"FAISS GPUs detected: {n_gpus}. Using GPU index.\")\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.GpuIndexFlatIP(res, dim)  # Inner Product (ok si normalize_embeddings=True)\n",
    "else:\n",
    "    print(\"FAISS GPU not available, using CPU index.\")\n",
    "    index = faiss.IndexFlatIP(dim)  # Inner Product (ok si normalize_embeddings=True)\n",
    "\n",
    "index.add(emb)\n",
    "\n",
    "\n",
    "def retrieve(query: str, k=3):\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, ids = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], ids[0]):\n",
    "        results.append((int(idx), float(score), chunks[int(idx)]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# (Optionnel) Login HF ici si besoin (sinon géré dans la cellule modèle)\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0a280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input CSV: ..\\Set évaluation RAG.csv\n",
      "Output CSV: Set évaluation RAG_QA.csv\n",
      "[1/12] 3.18s | id=1 | According to Rule 32, what are the precise definitions of a \"whistle,\" a \"short \n",
      "[2/12] 3.12s | id=2 | What are the minimum visibility ranges for the masthead light, sidelight, and st\n",
      "[3/12] 4.51s | id=3 | Under Rule 10 regarding Traffic Separation Schemes, when is a vessel allowed to \n",
      "[4/12] 2.64s | id=4 | What precise sound signal must a vessel aground give if she is 100 meters or mor\n",
      "[5/12] 10.49s | id=5 | Summarize the actions a vessel must take to determine if a \"risk of collision\" e\n",
      "[6/12] 8.26s | id=6 | Describe the lighting configuration required for a vessel \"not under command\" (N\n",
      "[7/12] 13.24s | id=7 | Explain the hierarchy of responsibilities between different vessel types (Power-\n",
      "[8/12] 5.19s | id=8 | Based on the \"IMO Recommendation on Navigational Watchkeeping,\" what are the key\n",
      "[9/12] 5.97s | id=9 | A sailing vessel is overtaking a power-driven vessel. According to the interacti\n",
      "[10/12] 4.11s | id=10 | A vessel detects another vessel by radar alone in restricted visibility and dete\n",
      "[11/12] 11.40s | id=11 | A vessel is \"restricted in her ability to manoeuvre\" and is engaged in a towing \n",
      "[12/12] 4.81s | id=12 | In a \"crossing situation\" (Rule 15), if the stand-on vessel finds herself so clo\n",
      "Done. Wrote: Set évaluation RAG_QA.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Batch Q/A from CSV -> minimal CSV (id, question, answer)\n",
    "# =========================\n",
    "# Pré-requis:\n",
    "# - Cellule 3: pdf_path pointe sur COLREG\n",
    "# - Cellule 4: chunks/raw_text créés\n",
    "# - Cellule 5: index + retrieve() OK\n",
    "# - Cellule 2: modèle + ask_pdf() OK\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def _resolve_csv_path(name: str) -> str:\n",
    "    candidates = [\n",
    "        Path(name),\n",
    "        Path.cwd() / name,\n",
    "        Path(\"..\") / name,\n",
    "        Path(\"../..\") / name,\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.is_file():\n",
    "            return str(p)\n",
    "    raise FileNotFoundError(f\"CSV not found: {name} (tried: {[str(c) for c in candidates]})\")\n",
    "\n",
    "\n",
    "def _pick_row_id(row: dict, fallback: int) -> str:\n",
    "    for key in (\"id\", \"Id\", \"ID\", \"question_id\", \"QuestionID\", \"questionId\"):\n",
    "        if key in row and str(row[key]).strip():\n",
    "            return str(row[key]).strip()\n",
    "    return str(fallback)\n",
    "\n",
    "\n",
    "csv_path = _resolve_csv_path(\"Set évaluation RAG.csv\")\n",
    "output_path = \"Set évaluation RAG_QA.csv\"\n",
    "\n",
    "print(\"Input CSV:\", csv_path)\n",
    "print(\"Output CSV:\", output_path)\n",
    "\n",
    "# Warmup to stabilize first-run latency\n",
    "_ = ask_pdf(\"Warmup question\", k=1, max_new_tokens=8)\n",
    "\n",
    "rows = []\n",
    "with open(csv_path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    if not reader.fieldnames:\n",
    "        raise ValueError(\"CSV has no header / fieldnames\")\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "if not rows:\n",
    "    raise ValueError(\"CSV contains no rows\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=[\"id\", \"question\", \"answer\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, row in enumerate(rows, start=1):\n",
    "        q = (\n",
    "            row.get(\"question\")\n",
    "            or row.get(\"Question\")\n",
    "            or row.get(\"QUESTION\")\n",
    "            or next(iter(row.values()))\n",
    "        )\n",
    "        q = (q or \"\").strip()\n",
    "        qid = _pick_row_id(row, fallback=i)\n",
    "\n",
    "        if not q:\n",
    "            writer.writerow({\"id\": qid, \"question\": \"\", \"answer\": \"\"})\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            answer, _sources = ask_pdf(q, k=5, max_new_tokens=512)\n",
    "            elapsed = time.perf_counter() - t0\n",
    "            print(f\"[{i}/{len(rows)}] {elapsed:.2f}s | id={qid} | {q[:80]}\")\n",
    "            writer.writerow({\"id\": qid, \"question\": q, \"answer\": answer})\n",
    "        except Exception as e:\n",
    "            elapsed = time.perf_counter() - t0\n",
    "            print(f\"[{i}/{len(rows)}] {elapsed:.2f}s | id={qid} | ERROR | {q[:80]} -> {e}\")\n",
    "            writer.writerow({\"id\": qid, \"question\": q, \"answer\": f\"Error: {e}\"})\n",
    "\n",
    "print(\"Done. Wrote:\", output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
