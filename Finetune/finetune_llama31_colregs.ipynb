{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths (Colab-friendly) ---\n",
    "# Assumption: you uploaded/copied the files into the current working directory (usually /content).\n",
    "DATA_JSON = Path('donnees_synthetiques.json')\n",
    "PDF_PATH = Path('COLREG-Consolidated-2018.pdf')\n",
    "\n",
    "OUT_DIR = Path('finetune_out')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Training data choice ---\n",
    "# If True: rebuild contexts using your chunking+retrieval (recommended for RAG).\n",
    "# If False: use the dataset's provided `context` field directly.\n",
    "USE_RETRIEVED_CONTEXT = True\n",
    "\n",
    "# Retrieval/chunking (keep aligned with your evaluation notebook defaults)\n",
    "CHUNK_TOKENS = 1024\n",
    "CHUNK_OVERLAP_TOKENS = 256\n",
    "RETRIEVE_K = 6\n",
    "BM25_WEIGHT = 0.55\n",
    "SEM_WEIGHT = 0.45\n",
    "EMBEDDER_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# --- Base model (HF) ---\n",
    "# You need access to Llama 3.1 weights on Hugging Face (and an HF token).\n",
    "BASE_MODEL_ID = os.getenv('BASE_MODEL_ID', 'meta-llama/Meta-Llama-3.1-8B-Instruct')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')  # set in your environment if required\n",
    "\n",
    "# --- SFT/LoRA hyperparams (start small) ---\n",
    "MAX_TRAIN_SAMPLES = None  # set e.g. 500 for a quick dry-run\n",
    "TRAIN_SPLIT = 0.95\n",
    "SEED = 42\n",
    "\n",
    "MAX_SEQ_LEN = 2048\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM = 8\n",
    "LR = 2e-4\n",
    "EPOCHS = 1\n",
    "\n",
    "print('CWD:', Path('.').resolve())\n",
    "print('DATA_JSON:', DATA_JSON.resolve() if DATA_JSON.exists() else DATA_JSON)\n",
    "print('PDF_PATH:', PDF_PATH.resolve() if PDF_PATH.exists() else PDF_PATH)\n",
    "print('OUT_DIR:', OUT_DIR.resolve())\n",
    "print('USE_RETRIEVED_CONTEXT:', USE_RETRIEVED_CONTEXT)\n",
    "print('BASE_MODEL_ID:', BASE_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be847720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(DATA_JSON, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = data.get('dataset', data)\n",
    "if not isinstance(rows, list):\n",
    "    raise ValueError('Expected a list at key dataset or at root')\n",
    "\n",
    "# Basic validation\n",
    "clean = []\n",
    "for r in rows:\n",
    "    if not isinstance(r, dict):\n",
    "        continue\n",
    "    q = (r.get('question') or '').strip()\n",
    "    a = (r.get('answer') or '').strip()\n",
    "    c = (r.get('context') or '').strip()\n",
    "    if q and a:\n",
    "        clean.append({'question': q, 'answer': a, 'context': c})\n",
    "\n",
    "if MAX_TRAIN_SAMPLES:\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(clean)\n",
    "    clean = clean[: int(MAX_TRAIN_SAMPLES)]\n",
    "\n",
    "print('Loaded samples:', len(clean))\n",
    "print('Example keys:', list(clean[0].keys()) if clean else None)\n",
    "print('Example question:', clean[0]['question'] if clean else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbb760",
   "metadata": {},
   "source": [
    "## (Optional) Build a retrieval index from the COLREG PDF\n",
    "This block recreates a lightweight version of your evaluation notebook retrieval stack: PDF → chunking → BM25 (+ optional embeddings).\n",
    "\n",
    "Colab assumption: `COLREG-Consolidated-2018.pdf` is in the current working directory.\n",
    "\n",
    "If `USE_RETRIEVED_CONTEXT = False`, you can skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PDF extraction\n",
    "try:\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pypdf'])\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "\n",
    "# BM25\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'rank-bm25'])\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "except Exception:\n",
    "    SentenceTransformer = None  # type: ignore\n",
    "\n",
    "_WORD_RE = re.compile(r'\\b\\w+\\b', re.UNICODE)\n",
    "_STOPWORDS = {\n",
    "    # EN\n",
    "    'the','a','an','and','or','to','of','in','on','for','with','is','are','was','were','be','as','at','by','it','this','that','from','into','over','under','not',\n",
    "    # FR\n",
    "    'le','la','les','un','une','des','et','ou','de','du','dans','sur','pour','avec','est','sont','été','être','ce','cet','cette','ces','pas','plus',\n",
    "}\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    words = [w.lower() for w in _WORD_RE.findall(text or '')]\n",
    "    return [w for w in words if len(w) >= 2 and w not in _STOPWORDS]\n",
    "\n",
    "def _normalize_ws(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', (text or '')).strip()\n",
    "\n",
    "def _pdf_bytes_to_text(data: bytes) -> str:\n",
    "    reader = PdfReader(io.BytesIO(data))\n",
    "    parts: List[str] = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            parts.append(page.extract_text() or '')\n",
    "        except Exception:\n",
    "            parts.append('')\n",
    "    return '\\n'.join(parts).strip()\n",
    "\n",
    "def load_pdf_from_path(path: str) -> str:\n",
    "    with open(path, 'rb') as f:\n",
    "        return _pdf_bytes_to_text(f.read())\n",
    "\n",
    "_COLREG_HEAD_RE = re.compile(r'(?im)^(rule\\s+\\d+\\b.*|annex\\s+[ivx]+\\b.*|appendix\\b.*)$')\n",
    "\n",
    "def _slice_tokens(text: str, *, max_tokens: int, overlap_tokens: int) -> List[str]:\n",
    "    text = _normalize_ws(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    toks = text.split()\n",
    "    out: List[str] = []\n",
    "    start = 0\n",
    "    n = len(toks)\n",
    "    max_tokens = max(1, int(max_tokens))\n",
    "    overlap_tokens = max(0, int(overlap_tokens))\n",
    "    while start < n:\n",
    "        end = min(n, start + max_tokens)\n",
    "        out.append(' '.join(toks[start:end]))\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = max(0, end - overlap_tokens)\n",
    "    return out\n",
    "\n",
    "def chunk_colreg(text: str, *, chunk_tokens: int, overlap_tokens: int) -> List[Tuple[str, str]]:\n",
    "    raw = (text or '').replace('\\r\\n', '\\n')\n",
    "    lines = [ln.strip() for ln in raw.split('\\n')]\n",
    "    headings: List[Tuple[int, str]] = []\n",
    "    for i, ln in enumerate(lines):\n",
    "        if not ln:\n",
    "            continue\n",
    "        m = _COLREG_HEAD_RE.match(ln)\n",
    "        if m:\n",
    "            headings.append((i, m.group(1).strip()))\n",
    "    if not headings:\n",
    "        joined = _normalize_ws(raw)\n",
    "        return [('Document', ch) for ch in _slice_tokens(joined, max_tokens=chunk_tokens, overlap_tokens=overlap_tokens)]\n",
    "\n",
    "    chunks: List[Tuple[str, str]] = []\n",
    "    for idx, (start_i, title) in enumerate(headings):\n",
    "        end_i = headings[idx + 1][0] if idx + 1 < len(headings) else len(lines)\n",
    "        section_text = _normalize_ws(' '.join([ln for ln in lines[start_i:end_i] if ln]))\n",
    "        if not section_text:\n",
    "            continue\n",
    "        for ch in _slice_tokens(section_text, max_tokens=chunk_tokens, overlap_tokens=overlap_tokens):\n",
    "            chunks.append((title, ch))\n",
    "    return chunks\n",
    "\n",
    "def _minmax(scores: np.ndarray) -> np.ndarray:\n",
    "    if scores.size == 0:\n",
    "        return scores\n",
    "    mn = float(scores.min())\n",
    "    mx = float(scores.max())\n",
    "    if mx - mn < 1e-9:\n",
    "        return np.zeros_like(scores, dtype=float)\n",
    "    return (scores - mn) / (mx - mn)\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    idx: int\n",
    "    section: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class AttachedPDF:\n",
    "    name: str\n",
    "    text: str\n",
    "    chunks: List[Chunk]\n",
    "    bm25: BM25Okapi\n",
    "    bm25_tokens: List[List[str]]\n",
    "    embedder_name: str\n",
    "    embeddings: Optional[np.ndarray]\n",
    "\n",
    "ATTACHED_PDF: Optional[AttachedPDF] = None\n",
    "_EMBEDDER: Optional['SentenceTransformer'] = None  # type: ignore\n",
    "\n",
    "def _get_embedder() -> Optional['SentenceTransformer']:\n",
    "    global _EMBEDDER\n",
    "    if SentenceTransformer is None:\n",
    "        return None\n",
    "    if _EMBEDDER is None:\n",
    "        _EMBEDDER = SentenceTransformer(EMBEDDER_NAME)\n",
    "    return _EMBEDDER\n",
    "\n",
    "def attach_pdf_text(name: str, text: str) -> None:\n",
    "    global ATTACHED_PDF\n",
    "    pairs = chunk_colreg(text, chunk_tokens=CHUNK_TOKENS, overlap_tokens=CHUNK_OVERLAP_TOKENS)\n",
    "    chunks = [Chunk(idx=i, section=sec, text=ch) for i, (sec, ch) in enumerate(pairs)]\n",
    "    bm25_tokens = [_tokenize(c.text) for c in chunks]\n",
    "    bm25 = BM25Okapi(bm25_tokens)\n",
    "\n",
    "    embedder = _get_embedder()\n",
    "    embeddings: Optional[np.ndarray] = None\n",
    "    if embedder is not None:\n",
    "        docs = [f'{c.section}. {c.text}' for c in chunks]\n",
    "        emb = embedder.encode(docs, normalize_embeddings=True, show_progress_bar=False)\n",
    "        embeddings = np.asarray(emb, dtype=np.float32)\n",
    "\n",
    "    ATTACHED_PDF = AttachedPDF(\n",
    "        name=name,\n",
    "        text=text,\n",
    "        chunks=chunks,\n",
    "        bm25=bm25,\n",
    "        bm25_tokens=bm25_tokens,\n",
    "        embedder_name=EMBEDDER_NAME,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f'Attached PDF: {name} ({len(text):,} chars, {len(chunks)} chunks)')\n",
    "    if embeddings is None:\n",
    "        print('(Embeddings disabled: sentence-transformers not available)')\n",
    "\n",
    "def _hybrid_scores(question: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert ATTACHED_PDF is not None\n",
    "    q_tokens = _tokenize(question)\n",
    "    bm25_scores = np.asarray(ATTACHED_PDF.bm25.get_scores(q_tokens), dtype=float)\n",
    "    bm25_norm = _minmax(bm25_scores)\n",
    "\n",
    "    sem_norm = np.zeros_like(bm25_norm, dtype=float)\n",
    "    if ATTACHED_PDF.embeddings is not None:\n",
    "        embedder = _get_embedder()\n",
    "        if embedder is not None:\n",
    "            q_emb = embedder.encode([question], normalize_embeddings=True, show_progress_bar=False)\n",
    "            q_emb = np.asarray(q_emb[0], dtype=np.float32)\n",
    "            sem = np.dot(ATTACHED_PDF.embeddings, q_emb).astype(float)\n",
    "            sem_norm = _minmax(sem)\n",
    "\n",
    "    hybrid = BM25_WEIGHT * bm25_norm + SEM_WEIGHT * sem_norm\n",
    "    return hybrid, bm25_norm, sem_norm\n",
    "\n",
    "def retrieve_context(question: str, *, k: int) -> List[int]:\n",
    "    if ATTACHED_PDF is None:\n",
    "        return []\n",
    "    hybrid, _, _ = _hybrid_scores(question)\n",
    "    ranked = np.argsort(-hybrid)[: max(20, k)].tolist()\n",
    "    ranked = [i for i in ranked if hybrid[i] > 0]\n",
    "    return ranked[:k]\n",
    "\n",
    "def build_excerpts(indices: List[int]) -> str:\n",
    "    assert ATTACHED_PDF is not None\n",
    "    if not indices:\n",
    "        return ''\n",
    "    blocks: List[str] = []\n",
    "    for idx in indices:\n",
    "        c = ATTACHED_PDF.chunks[idx]\n",
    "        blocks.append(f'(Chunk {idx+1}) {c.text}')\n",
    "    return '\\n'.join(blocks).strip()\n",
    "\n",
    "if USE_RETRIEVED_CONTEXT:\n",
    "    if not PDF_PATH.exists():\n",
    "        raise FileNotFoundError(f'Missing PDF: {PDF_PATH}')\n",
    "    text = load_pdf_from_path(str(PDF_PATH))\n",
    "    attach_pdf_text(PDF_PATH.name, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee20906",
   "metadata": {},
   "source": [
    "## Build the fine-tuning dataset (chat format)\n",
    "We will train the model to answer using **ONLY** the provided excerpts and to cite chunks like `(Chunk 12)`.\n",
    "\n",
    "This aligns with your evaluation strategy and encourages better grounded behavior during RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67280228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'You are a COLREGS assistant. Answer the QUESTION using ONLY the provided EXCERPTS. '\n",
    "    'If the EXCERPTS do not contain the answer, say you do not know. '\n",
    "    'Cite evidence using parentheses like (Chunk 12). '\n",
    "    'Do not invent rule text.'\n",
    ")\n",
    "\n",
    "def make_example(question: str, answer: str, *, context_text: str) -> dict:\n",
    "    user = (\n",
    "        'EXCERPTS:\\n'\n",
    "        + (context_text.strip() if context_text.strip() else '[No relevant excerpts found]')\n",
    "        + '\\n\\nQUESTION:\\n'\n",
    "        + question.strip()\n",
    "    )\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': user},\n",
    "            {'role': 'assistant', 'content': answer.strip()},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "examples = []\n",
    "for r in clean:\n",
    "    q = r['question']\n",
    "    a = r['answer']\n",
    "    if USE_RETRIEVED_CONTEXT:\n",
    "        idxs = retrieve_context(q, k=RETRIEVE_K)\n",
    "        ctx = build_excerpts(idxs)\n",
    "    else:\n",
    "        ctx = r.get('context', '')\n",
    "    examples.append(make_example(q, a, context_text=ctx))\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(examples)\n",
    "n_train = max(1, int(math.floor(len(examples) * TRAIN_SPLIT)))\n",
    "train_examples = examples[:n_train]\n",
    "eval_examples = examples[n_train:]\n",
    "\n",
    "train_path = OUT_DIR / 'train.jsonl'\n",
    "eval_path = OUT_DIR / 'eval.jsonl'\n",
    "\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    for ex in train_examples:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n",
    "with open(eval_path, 'w', encoding='utf-8') as f:\n",
    "    for ex in eval_examples:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Train examples:', len(train_examples))\n",
    "print('Eval examples:', len(eval_examples))\n",
    "print('Wrote:', train_path)\n",
    "print('Wrote:', eval_path)\n",
    "print('Sample user message (truncated):')\n",
    "print(train_examples[0]['messages'][1]['content'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e05b9e",
   "metadata": {},
   "source": [
    "## Fine-tune (Option A): Transformers + LoRA/QLoRA (recommended if you have an NVIDIA GPU)\n",
    "This trains a LoRA adapter on the HF base model, then merges it into full weights.\n",
    "\n",
    "If this fails on Windows (common causes: CUDA/bitsandbytes), skip to the **GGUF conversion** section after training elsewhere, or use a Linux environment/WSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab647818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "# NOTE: On Windows, GPU QLoRA may require a specific CUDA + PyTorch build.\n",
    "import sys, subprocess\n",
    "\n",
    "pkgs = [\n",
    "    'torch',\n",
    "    'transformers>=4.45.0',\n",
    "    'datasets>=2.19.0',\n",
    "    'accelerate>=0.34.0',\n",
    "    'peft>=0.12.0',\n",
    "    'trl>=0.11.0',\n",
    "]\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
    "\n",
    "# Optional (QLoRA). If it fails to install/use, the notebook will fall back to regular LoRA.\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'bitsandbytes'])\n",
    "    HAS_BNB = True\n",
    "except Exception:\n",
    "    HAS_BNB = False\n",
    "\n",
    "print('bitsandbytes available:', HAS_BNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig \n",
    "\n",
    "# --- Configuration Setup ---\n",
    "# (Assuming OUT_DIR, BASE_MODEL_ID, HF_TOKEN, etc., are defined above)\n",
    "\n",
    "# 1. Load Dataset\n",
    "ds = load_dataset('json', data_files={'train': str(train_path), 'eval': str(eval_path)})\n",
    "\n",
    "# 2. Tokenizer Setup\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "# Important for some models to ensure padding is on the correct side\n",
    "tok.padding_side = 'right' \n",
    "\n",
    "# 3. Model with QLoRA\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ") if (HAS_BNB and torch.cuda.is_available()) else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# 4. LoRA Configuration\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    ")\n",
    "\n",
    "# 5. Dataset Formatting\n",
    "def format_chat(example):\n",
    "    text = tok.apply_chat_template(example['messages'], tokenize=False, add_generation_prompt=False)\n",
    "    return {'text': text}\n",
    "\n",
    "train_ds = ds['train'].map(format_chat, remove_columns=ds['train'].column_names)\n",
    "eval_ds = ds['eval'].map(format_chat, remove_columns=ds['eval'].column_names) if 'eval' in ds else None\n",
    "\n",
    "# 6. SFTConfig (The \"Modern\" TrainingArguments)\n",
    "args = SFTConfig(\n",
    "    output_dir=str(OUT_DIR / 'runs'),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy='steps' if (eval_ds is not None) else 'no',\n",
    "    eval_steps=200 if (eval_ds is not None) else None,\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    "    # SFT Specific parameters moved into Config\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 7. Trainer Initialization\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tok, # Corrected keyword\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=lora,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "# 8. Run Training\n",
    "trainer.train()\n",
    "\n",
    "# 9. Save final adapter\n",
    "adapter_dir = OUT_DIR / 'lora_adapter'\n",
    "trainer.save_model(str(adapter_dir)) # trainer.save_model is cleaner than trainer.model.save_pretrained\n",
    "print('Saved LoRA adapter:', adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA into full weights (required before GGUF conversion)\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "merged_dir = OUT_DIR / 'merged_hf_model'\n",
    "merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='cpu',\n",
    ")\n",
    "peft = PeftModel.from_pretrained(base, str(adapter_dir))\n",
    "merged = peft.merge_and_unload()\n",
    "\n",
    "tok2 = AutoTokenizer.from_pretrained(BASE_MODEL_ID, token=HF_TOKEN)\n",
    "if tok2.pad_token is None:\n",
    "    tok2.pad_token = tok2.eos_token\n",
    "\n",
    "merged.save_pretrained(merged_dir, safe_serialization=True)\n",
    "tok2.save_pretrained(merged_dir)\n",
    "print('Merged HF model saved:', merged_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15c1b1",
   "metadata": {},
   "source": [
    "## Convert merged HF model to GGUF + quantize (llama.cpp)\n",
    "This produces a `.gguf` you can use with llama.cpp and (optionally) Ollama.\n",
    "\n",
    "This step clones and builds llama.cpp (needs `git`, `cmake`, and a C++ compiler installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e11f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "LLAMA_CPP_DIR = OUT_DIR / 'llama.cpp'\n",
    "\n",
    "if not LLAMA_CPP_DIR.exists():\n",
    "    subprocess.check_call(['git', 'clone', '--depth', '1', 'https://github.com/ggerganov/llama.cpp', str(LLAMA_CPP_DIR)])\n",
    "\n",
    "# Build (Windows): this uses CMake. You may need to adjust generator/toolchain.\n",
    "build_dir = LLAMA_CPP_DIR / 'build'\n",
    "build_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure\n",
    "subprocess.check_call(['cmake', '-S', str(LLAMA_CPP_DIR), '-B', str(build_dir)])\n",
    "# Build\n",
    "subprocess.check_call(['cmake', '--build', str(build_dir), '--config', 'Release'])\n",
    "\n",
    "print('llama.cpp built at:', build_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb202f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HF -> GGUF\n",
    "import subprocess\n",
    "\n",
    "# llama.cpp conversion script path can vary slightly by version\n",
    "convert_py = LLAMA_CPP_DIR / 'convert_hf_to_gguf.py'\n",
    "if not convert_py.exists():\n",
    "    # fallback for older layouts\n",
    "    convert_py = LLAMA_CPP_DIR / 'convert.py'\n",
    "\n",
    "if not convert_py.exists():\n",
    "    raise FileNotFoundError('Could not find llama.cpp HF->GGUF conversion script')\n",
    "\n",
    "gguf_f16 = OUT_DIR / 'colregs_rag_finetuned-f16.gguf'\n",
    "subprocess.check_call([\n",
    "    'python', str(convert_py),\n",
    "    str(merged_dir),\n",
    "    '--outfile', str(gguf_f16),\n",
    "])\n",
    "\n",
    "print('Wrote:', gguf_f16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da799f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q4_K_M (similar to your existing file)\n",
    "import subprocess\n",
    "\n",
    "# quantize binary is typically in build/bin/quantize (or build/Release/quantize.exe on Windows)\n",
    "quant_candidates = [\n",
    "    OUT_DIR / 'llama.cpp' / 'build' / 'bin' / 'llama-quantize',\n",
    "    OUT_DIR / 'llama.cpp' / 'build' / 'bin' / 'quantize.exe',\n",
    "    OUT_DIR / 'llama.cpp' / 'build' / 'Release' / 'quantize.exe',\n",
    "]\n",
    "quant_bin = next((p for p in quant_candidates if p.exists()), None)\n",
    "if quant_bin is None:\n",
    "    raise FileNotFoundError('Could not locate llama.cpp quantize binary. Check your build output.')\n",
    "\n",
    "gguf_q4 = OUT_DIR / 'colregs_rag_finetuned-Q4_K_M.gguf'\n",
    "subprocess.check_call([str(quant_bin), str(gguf_f16), str(gguf_q4), 'Q4_K_M'])\n",
    "print('Wrote:', gguf_q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760175fe",
   "metadata": {},
   "source": [
    "## Next: use the fine-tuned GGUF\n",
    "- For llama.cpp: point your runner to `colregs_rag_finetuned-Q4_K_M.gguf`\n",
    "- For Ollama: you can create a new model using a Modelfile that references the GGUF (Ollama support depends on your version).\n",
    "\n",
    "If you want, I can also adapt your existing evaluation notebook to automatically switch between the base model and the fine-tuned GGUF for side-by-side RAG evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
