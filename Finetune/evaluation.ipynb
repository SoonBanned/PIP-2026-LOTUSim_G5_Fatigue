{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9606e0f3",
   "metadata": {},
   "source": [
    "# Local Ollama Chatbot (Llama 3.1)\n",
    "\n",
    "This notebook runs a minimal chatbot against a **local** Ollama model via its OpenAI-compatible API.\n",
    "\n",
    "Prereq:\n",
    "- Install Ollama and run it\n",
    "- Ensure a model is pulled (example: `ollama pull llama3.1:latest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe6dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_OPENAI_BASE_URL: http://localhost:11434/v1\n",
      "MODEL_NAME: llama3.1_fine:latest\n"
     ]
    }
   ],
   "source": [
    "# Local backend setup (Ollama)\n",
    "import os\n",
    "\n",
    "# Ollama OpenAI-compatible base URL\n",
    "os.environ.setdefault(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\")\n",
    "\n",
    "# Model to use (set LOCAL_MODEL_NAME to override)\n",
    "MODEL_NAME = os.getenv(\"LOCAL_MODEL_NAME\", \"llama3.1_fine:latest\")\n",
    "\n",
    "# Optional: some gateways require a key; Ollama accepts any value.\n",
    "# os.environ.setdefault(\"LOCAL_OPENAI_API_KEY\", \"ollama\")\n",
    "\n",
    "print(\"LOCAL_OPENAI_BASE_URL:\", os.getenv(\"LOCAL_OPENAI_BASE_URL\"))\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89c805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:11434/v1/models -> 200\n",
      "{\"object\":\"list\",\"data\":[{\"id\":\"llama3.1_fine:latest\",\"object\":\"model\",\"created\":1768215487,\"owned_by\":\"library\"},{\"id\":\"llama3.1:latest\",\"object\":\"model\",\"created\":1767698163,\"owned_by\":\"library\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connectivity check: local Ollama OpenAI-compatible API\n",
    "import os, requests\n",
    "\n",
    "base = os.getenv(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\").rstrip(\"/\")\n",
    "url = f\"{base}/models\"\n",
    "r = requests.get(url, timeout=10)\n",
    "print(\"GET\", url, \"->\", r.status_code)\n",
    "print((r.text or \"\")[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11687c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST http://localhost:11434/v1/chat/completions -> 200\n",
      "{\"id\":\"chatcmpl-361\",\"object\":\"chat.completion\",\"created\":1768219200,\"model\":\"llama3.1_fine:latest\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today?\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":16,\"completion_to\n"
     ]
    }
   ],
   "source": [
    "# Test local chat completions (optional)\n",
    "import os, requests, json\n",
    "\n",
    "base = os.getenv(\"LOCAL_OPENAI_BASE_URL\", \"http://localhost:11434/v1\").rstrip(\"/\")\n",
    "url = f\"{base}/chat/completions\"\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n",
    "    \"max_tokens\": 32,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "print(\"POST\", url, \"->\", resp.status_code)\n",
    "print((resp.text or \"\")[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520e5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "LOCAL_OPENAI_BASE_URL: http://localhost:11434/v1\n",
      "MODEL_NAME: llama3.1_fine:latest\n",
      "Try: bot.reply('Hello!')\n"
     ]
    }
   ],
   "source": [
    "import importlib, os\n",
    "\n",
    "import deepseek_chat\n",
    "importlib.reload(deepseek_chat)\n",
    "\n",
    "from deepseek_chat import DeepSeekChatbot\n",
    "\n",
    "bot = DeepSeekChatbot(model=MODEL_NAME)\n",
    "print(\"Ready.\")\n",
    "print(\"LOCAL_OPENAI_BASE_URL:\", os.getenv(\"LOCAL_OPENAI_BASE_URL\"))\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"Try: bot.reply('Hello!')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20048336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Smoke test\n",
    "print(bot.reply(\"Say hello in one sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740514",
   "metadata": {},
   "source": [
    "# Finetuned model evaluation (same training prompts)\n",
    "\n",
    "This section evaluates the fine-tuned Ollama model (`llama3.1_fine`) on the **same 3 tasks** used during training:\n",
    "\n",
    "1. `quiz_generate` (schema check + exact-match vs dataset)\n",
    "2. `grade_truth_answer` (score match vs dataset)\n",
    "3. `rule_application_grade` (score match vs dataset)\n",
    "\n",
    "Important: the prompts below copy the **exact** `SYSTEM_PROMPT_*` and `TASK:`-formatted user templates from the fine-tuning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf85dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\quizz_gen.json\n",
      "Grade: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\grade_dataset.json\n",
      "Scenario: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\scenario_dataset.json\n",
      "Loaded: 399 quiz; 192 grade; 291 scenario\n"
     ]
    }
   ],
   "source": [
    "# --- Prompt templates copied from fine-tuning notebook ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT_QUIZ = (\n",
    "    \"You generate COLREGS quiz items. \"\n",
    "    \"Use ONLY the provided quotes as evidence; do not invent rule text.\"\n",
    " )\n",
    "\n",
    "SYSTEM_PROMPT_GRADE_TRUTH = (\n",
    "    \"You are a strict grader for COLREGS answers. \"\n",
    "    \"Grade the STUDENT_ANSWER against the TRUTH_ANSWER using the RUBRIC. \"\n",
    "    \"Do not add external facts.\"\n",
    " )\n",
    "\n",
    "SYSTEM_PROMPT_GRADE_SCENARIO = (\n",
    "    \"You are a strict grader for COLREGS scenario rule-application. \"\n",
    "    \"Grade the STUDENT_ANSWER against the REFERENCE_APPLICATION text. \"\n",
    "    \"Do not add external facts.\"\n",
    " )\n",
    "\n",
    "\n",
    "def _find_existing_path(*candidates: str) -> Path:\n",
    "    for c in candidates:\n",
    "        p = Path(c)\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    raise FileNotFoundError(f\"None of these paths exist: {candidates}\")\n",
    "\n",
    "\n",
    "def _format_quotes(quotes: list[dict]) -> str:\n",
    "    if not quotes:\n",
    "        return \"[No quotes provided]\"\n",
    "    lines = []\n",
    "    for q in quotes:\n",
    "        if not isinstance(q, dict):\n",
    "            continue\n",
    "        ref = (q.get(\"ref\") or \"\").strip()\n",
    "        qt = (q.get(\"quote\") or \"\").strip()\n",
    "        if qt:\n",
    "            if ref:\n",
    "                lines.append(f\"- ({ref}) {qt}\")\n",
    "            else:\n",
    "                lines.append(f\"- {qt}\")\n",
    "    return \"\\n\".join(lines) if lines else \"[No quotes provided]\"\n",
    "\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split()).strip()\n",
    "\n",
    "\n",
    "def _extract_first_json_object(text: str) -> dict[str, Any]:\n",
    "    \"\"\"Best-effort parse: accept raw JSON, or extract first {...} block.\"\"\"\n",
    "    s = (text or \"\").strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"Empty model output\")\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start >= 0 and end > start:\n",
    "        candidate = s[start : end + 1]\n",
    "        obj = json.loads(candidate)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    raise ValueError(\"Could not parse JSON object from model output\")\n",
    "\n",
    "\n",
    "def build_user_prompt_quiz(r: dict[str, Any]) -> str:\n",
    "    quotes = r.get(\"supporting_quotes\") or []\n",
    "    if not isinstance(quotes, list):\n",
    "        quotes = []\n",
    "    difficulty = (r.get(\"difficulty\") or \"\").strip()\n",
    "    rule_refs = r.get(\"rule_refs\") or []\n",
    "    if not isinstance(rule_refs, list):\n",
    "        rule_refs = []\n",
    "    return (\n",
    "        \"TASK: quiz_generate\\n\"\n",
    "        + (f\"DIFFICULTY: {difficulty}\\n\" if difficulty else \"\")\n",
    "        + (\"RULE_REFS: \" + \", \".join([str(x) for x in rule_refs if str(x).strip()]) + \"\\n\" if rule_refs else \"\")\n",
    "        + \"QUOTES:\\n\" + _format_quotes(quotes) + \"\\n\\n\"\n",
    "        + \"Generate ONE quiz item as JSON with keys: question, truth_answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_user_prompt_grade_truth(r: dict[str, Any], ua: dict[str, Any]) -> str:\n",
    "    q = (r.get(\"question\") or \"\").strip()\n",
    "    truth = (r.get(\"truth_answer\") or \"\").strip()\n",
    "    rubric = r.get(\"rubric\") or []\n",
    "    support = r.get(\"supporting_quotes\") or []\n",
    "    if not isinstance(rubric, list):\n",
    "        rubric = []\n",
    "    if not isinstance(support, list):\n",
    "        support = []\n",
    "    student = (ua.get(\"answer\") or \"\").strip()\n",
    "\n",
    "    rubric_lines: list[str] = []\n",
    "    for it in rubric:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        rid = (it.get(\"id\") or \"\").strip()\n",
    "        mx = it.get(\"max\")\n",
    "        crit = (it.get(\"criterion\") or \"\").strip()\n",
    "        if rid and crit and isinstance(mx, (int, float)):\n",
    "            rubric_lines.append(f\"- {rid} (max {int(mx)}): {crit}\")\n",
    "\n",
    "    return (\n",
    "        \"TASK: grade_truth_answer\\n\"\n",
    "        + \"QUESTION:\\n\" + q + \"\\n\\n\"\n",
    "        + \"TRUTH_ANSWER:\\n\" + truth + \"\\n\\n\"\n",
    "        + \"RUBRIC (sum to 10):\\n\" + \"\\n\".join(rubric_lines) + \"\\n\\n\"\n",
    "        + \"SUPPORTING_QUOTES:\\n\" + _format_quotes(support) + \"\\n\\n\"\n",
    "        + \"STUDENT_ANSWER:\\n\" + student + \"\\n\\n\"\n",
    "        + \"Return ONLY JSON in the schema: {\\\"score_total\\\": int, \\\"breakdown\\\": [...], \\\"missing_points\\\": [...], \\\"incorrect_claims\\\": [...], \\\"feedback\\\": str}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_user_prompt_grade_scenario(r: dict[str, Any]) -> str:\n",
    "    # scenario_dataset.json includes the reference text already\n",
    "    scenario = (r.get(\"scenario\") or \"\").strip()\n",
    "    source_id = str(r.get(\"source_id\") or \"\").strip()\n",
    "    rule_refs = r.get(\"rule_refs\") or []\n",
    "    if not isinstance(rule_refs, list):\n",
    "        rule_refs = []\n",
    "    reference_application = (r.get(\"reference_application\") or \"\").strip()\n",
    "    student = (r.get(\"student_answer\") or \"\").strip()\n",
    "    return (\n",
    "        \"TASK: rule_application_grade\\n\"\n",
    "        + f\"SOURCE_ID: {source_id}\\n\"\n",
    "        + (\"RULE_REFS: \" + \", \".join([str(x) for x in rule_refs if str(x).strip()]) + \"\\n\" if rule_refs else \"\")\n",
    "        + \"SCENARIO:\\n\" + scenario + \"\\n\\n\"\n",
    "        + \"REFERENCE_APPLICATION:\\n\" + reference_application + \"\\n\\n\"\n",
    "        + \"STUDENT_ANSWER:\\n\" + student + \"\\n\\n\"\n",
    "        + \"Return ONLY JSON: {\\\"score\\\": int} (0..10).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_json(path: Path) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# Locate datasets (this notebook lives in Finetune/, but CWD can vary)\n",
    "QUIZ_JSON = _find_existing_path(\n",
    "    \"quizz_gen.json\",\n",
    "    str(Path(\"Finetune\") / \"quizz_gen.json\"),\n",
    "    str(Path(\"..\") / \"Finetune\" / \"quizz_gen.json\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\quizz_gen.json\",\n",
    ")\n",
    "GRADE_JSON = _find_existing_path(\n",
    "    \"grade_dataset.json\",\n",
    "    str(Path(\"Finetune\") / \"grade_dataset.json\"),\n",
    "    str(Path(\"..\") / \"Finetune\" / \"grade_dataset.json\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\grade_dataset.json\",\n",
    ")\n",
    "SCENARIO_JSON = _find_existing_path(\n",
    "    \"scenario_dataset.json\",\n",
    "    str(Path(\"Finetune\") / \"scenario_dataset.json\"),\n",
    "    str(Path(\"..\") / \"Finetune\" / \"scenario_dataset.json\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\scenario_dataset.json\",\n",
    ")\n",
    "\n",
    "print(\"Quiz:\", QUIZ_JSON)\n",
    "print(\"Grade:\", GRADE_JSON)\n",
    "print(\"Scenario:\", SCENARIO_JSON)\n",
    "\n",
    "quiz_rows = _load_json(Path(QUIZ_JSON))\n",
    "grade_rows = _load_json(Path(GRADE_JSON))\n",
    "scenario_rows = _load_json(Path(SCENARIO_JSON))\n",
    "\n",
    "assert isinstance(quiz_rows, list)\n",
    "assert isinstance(grade_rows, list)\n",
    "assert isinstance(scenario_rows, list)\n",
    "print(\"Loaded:\", len(quiz_rows), \"quiz;\", len(grade_rows), \"grade;\", len(scenario_rows), \"scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f94b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario sources: 209\n"
     ]
    }
   ],
   "source": [
    "# Load scenario reference text (ID -> Application_Regle)\n",
    "SCENARIO_SOURCE_JSON = _find_existing_path(\n",
    "    \"colreg_v9_split.json\",\n",
    "    str(Path(\"Finetune\") / \"colreg_v9_split.json\"),\n",
    "    str(Path(\"..\") / \"Finetune\" / \"colreg_v9_split.json\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Finetune\\colreg_v9_split.json\",\n",
    ")\n",
    "scenario_source_rows = _load_json(Path(SCENARIO_SOURCE_JSON))\n",
    "assert isinstance(scenario_source_rows, list)\n",
    "\n",
    "application_by_id: dict[str, str] = {}\n",
    "for row in scenario_source_rows:\n",
    "    if not isinstance(row, dict):\n",
    "        continue\n",
    "    sid = str(row.get(\"ID\") or \"\").strip()\n",
    "    app = (row.get(\"Application_Regle\") or \"\").strip()\n",
    "    if sid and app:\n",
    "        application_by_id[sid] = app\n",
    "\n",
    "print(\"Scenario sources:\", len(application_by_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11e5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch evaluator (calls Ollama via DeepSeekChatbot) ---\n",
    "import math\n",
    "\n",
    "\n",
    "def _call_model_json(*, system_prompt: str, user_prompt: str, max_tokens: int = 512) -> tuple[str, dict[str, Any] | None, str | None]:\n",
    "    \"\"\"Returns (raw_text, parsed_json_or_none, error_or_none).\"\"\"\n",
    "    bot.reset()\n",
    "    raw = bot.reply(\n",
    "        user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    try:\n",
    "        obj = _extract_first_json_object(raw)\n",
    "        return raw, obj, None\n",
    "    except Exception as e:\n",
    "        return raw, None, str(e)\n",
    "\n",
    "\n",
    "def evaluate_quiz_generate(*, n: int = 30, seed: int = 0) -> dict[str, Any]:\n",
    "    rows = [r for r in quiz_rows if isinstance(r, dict)]\n",
    "    random.Random(seed).shuffle(rows)\n",
    "    rows = rows[: max(0, int(n))]\n",
    "\n",
    "    ok = 0\n",
    "    parse_fail = 0\n",
    "    exact_match = 0\n",
    "    samples: list[dict[str, Any]] = []\n",
    "\n",
    "    for r in rows:\n",
    "        user = build_user_prompt_quiz(r)\n",
    "        raw, obj, err = _call_model_json(system_prompt=SYSTEM_PROMPT_QUIZ, user_prompt=user, max_tokens=256)\n",
    "        if obj is None:\n",
    "            parse_fail += 1\n",
    "            samples.append({\"task\": \"quiz_generate\", \"error\": err, \"raw\": raw[:800]})\n",
    "            continue\n",
    "        ok += 1\n",
    "        q_pred = _normalize_text(str(obj.get(\"question\", \"\")))\n",
    "        t_pred = _normalize_text(str(obj.get(\"truth_answer\", \"\")))\n",
    "        q_gt = _normalize_text(str(r.get(\"question\", \"\")))\n",
    "        t_gt = _normalize_text(str(r.get(\"truth_answer\", \"\")))\n",
    "        if q_pred == q_gt and t_pred == t_gt:\n",
    "            exact_match += 1\n",
    "        if len(samples) < 3:\n",
    "            samples.append({\n",
    "                \"task\": \"quiz_generate\",\n",
    "                \"gt\": {\"question\": q_gt, \"truth_answer\": t_gt},\n",
    "                \"pred\": {\"question\": q_pred, \"truth_answer\": t_pred},\n",
    "            })\n",
    "\n",
    "    total = len(rows)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"parsed_ok\": ok,\n",
    "        \"parse_fail\": parse_fail,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"exact_match_rate\": (exact_match / total) if total else None,\n",
    "        \"samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_grade_truth_answer(*, n_questions: int = 50, seed: int = 0) -> dict[str, Any]:\n",
    "    rows = [r for r in grade_rows if isinstance(r, dict)]\n",
    "    random.Random(seed).shuffle(rows)\n",
    "    rows = rows[: max(0, int(n_questions))]\n",
    "\n",
    "    total = 0\n",
    "    parsed_ok = 0\n",
    "    parse_fail = 0\n",
    "    score_match = 0\n",
    "    abs_errs: list[float] = []\n",
    "    samples: list[dict[str, Any]] = []\n",
    "\n",
    "    for r in rows:\n",
    "        uas = r.get(\"user_answers\") or []\n",
    "        if not isinstance(uas, list) or not uas:\n",
    "            continue\n",
    "        # pick one answer per question to keep eval cost reasonable\n",
    "        ua = random.Random(seed + total).choice([x for x in uas if isinstance(x, dict)] or [{}])\n",
    "        grade = ua.get(\"grade\") or {}\n",
    "        if not isinstance(grade, dict):\n",
    "            continue\n",
    "        gt_score = grade.get(\"score_total\")\n",
    "        if not isinstance(gt_score, int):\n",
    "            continue\n",
    "\n",
    "        user = build_user_prompt_grade_truth(r, ua)\n",
    "        raw, obj, err = _call_model_json(system_prompt=SYSTEM_PROMPT_GRADE_TRUTH, user_prompt=user, max_tokens=512)\n",
    "        total += 1\n",
    "        if obj is None:\n",
    "            parse_fail += 1\n",
    "            if len(samples) < 3:\n",
    "                samples.append({\"task\": \"grade_truth_answer\", \"gt_score_total\": gt_score, \"error\": err, \"raw\": raw[:800]})\n",
    "            continue\n",
    "        parsed_ok += 1\n",
    "        pred_score = obj.get(\"score_total\")\n",
    "        if isinstance(pred_score, bool):\n",
    "            pred_score = int(pred_score)\n",
    "        if isinstance(pred_score, (int, float)) and float(pred_score).is_integer():\n",
    "            pred_score = int(pred_score)\n",
    "        else:\n",
    "            pred_score = None\n",
    "        if pred_score is not None:\n",
    "            if pred_score == gt_score:\n",
    "                score_match += 1\n",
    "            abs_errs.append(abs(float(pred_score) - float(gt_score)))\n",
    "        if len(samples) < 3:\n",
    "            samples.append({\"task\": \"grade_truth_answer\", \"gt_score_total\": gt_score, \"pred_score_total\": pred_score})\n",
    "\n",
    "    mae = (sum(abs_errs) / len(abs_errs)) if abs_errs else None\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"parsed_ok\": parsed_ok,\n",
    "        \"parse_fail\": parse_fail,\n",
    "        \"score_match\": score_match,\n",
    "        \"score_match_rate\": (score_match / total) if total else None,\n",
    "        \"mae_score_total\": mae,\n",
    "        \"samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def _flatten_scenario_rows() -> list[dict[str, Any]]:\n",
    "    flat: list[dict[str, Any]] = []\n",
    "    for r in scenario_rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        scenario = (r.get(\"scenario\") or \"\").strip()\n",
    "        source_id = str(r.get(\"source_id\") or \"\").strip()\n",
    "        rule_refs = r.get(\"rule_refs\") or []\n",
    "        if not isinstance(rule_refs, list):\n",
    "            rule_refs = []\n",
    "        ref_app = (r.get(\"reference_application\") or \"\").strip()\n",
    "        answers = r.get(\"answers\") or []\n",
    "        if not scenario or not source_id or not ref_app or not isinstance(answers, list):\n",
    "            continue\n",
    "        for a in answers:\n",
    "            if not isinstance(a, dict):\n",
    "                continue\n",
    "            student = (a.get(\"answer\") or \"\").strip()\n",
    "            score = a.get(\"score\")\n",
    "            if not student or not isinstance(score, int):\n",
    "                continue\n",
    "            flat.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"source_id\": source_id,\n",
    "                \"rule_refs\": rule_refs,\n",
    "                \"reference_application\": ref_app,\n",
    "                \"student_answer\": student,\n",
    "                \"score\": score,\n",
    "            })\n",
    "    return flat\n",
    "\n",
    "\n",
    "def evaluate_rule_application_grade(*, n: int = 120, seed: int = 0) -> dict[str, Any]:\n",
    "    rows = _flatten_scenario_rows()\n",
    "    random.Random(seed).shuffle(rows)\n",
    "    rows = rows[: max(0, int(n))]\n",
    "\n",
    "    total = 0\n",
    "    parsed_ok = 0\n",
    "    parse_fail = 0\n",
    "    score_match = 0\n",
    "    abs_errs: list[float] = []\n",
    "    samples: list[dict[str, Any]] = []\n",
    "\n",
    "    for r in rows:\n",
    "        user = build_user_prompt_grade_scenario(r)\n",
    "        raw, obj, err = _call_model_json(system_prompt=SYSTEM_PROMPT_GRADE_SCENARIO, user_prompt=user, max_tokens=128)\n",
    "        total += 1\n",
    "        if obj is None:\n",
    "            parse_fail += 1\n",
    "            if len(samples) < 3:\n",
    "                samples.append({\"task\": \"rule_application_grade\", \"gt_score\": r[\"score\"], \"error\": err, \"raw\": raw[:800]})\n",
    "            continue\n",
    "        parsed_ok += 1\n",
    "        pred_score = obj.get(\"score\")\n",
    "        if isinstance(pred_score, bool):\n",
    "            pred_score = int(pred_score)\n",
    "        if isinstance(pred_score, (int, float)) and float(pred_score).is_integer():\n",
    "            pred_score = int(pred_score)\n",
    "        else:\n",
    "            pred_score = None\n",
    "        if pred_score is not None:\n",
    "            if pred_score == r[\"score\"]:\n",
    "                score_match += 1\n",
    "            abs_errs.append(abs(float(pred_score) - float(r[\"score\"])))\n",
    "        if len(samples) < 3:\n",
    "            samples.append({\"task\": \"rule_application_grade\", \"gt_score\": r[\"score\"], \"pred_score\": pred_score})\n",
    "\n",
    "    mae = (sum(abs_errs) / len(abs_errs)) if abs_errs else None\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"parsed_ok\": parsed_ok,\n",
    "        \"parse_fail\": parse_fail,\n",
    "        \"score_match\": score_match,\n",
    "        \"score_match_rate\": (score_match / total) if total else None,\n",
    "        \"mae_score\": mae,\n",
    "        \"samples\": samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e133a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override scenario eval to match training prompt (inject REFERENCE_APPLICATION from colreg_v9_split.json)\n",
    "def _flatten_scenario_rows() -> list[dict[str, Any]]:\n",
    "    flat: list[dict[str, Any]] = []\n",
    "    for r in scenario_rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        scenario = (r.get(\"scenario\") or \"\").strip()\n",
    "        source_id_raw = r.get(\"source_id\")\n",
    "        source_id = str(source_id_raw).strip() if source_id_raw is not None else \"\"\n",
    "        rule_refs = r.get(\"rule_refs\") or []\n",
    "        if not isinstance(rule_refs, list):\n",
    "            rule_refs = []\n",
    "        answers = r.get(\"answers\") or []\n",
    "        if not scenario or not source_id or not isinstance(answers, list):\n",
    "            continue\n",
    "        ref_app = (application_by_id.get(source_id) or \"\").strip()\n",
    "        if not ref_app:\n",
    "            continue\n",
    "        for a in answers:\n",
    "            if not isinstance(a, dict):\n",
    "                continue\n",
    "            student = (a.get(\"answer\") or \"\").strip()\n",
    "            score = a.get(\"score\")\n",
    "            if not student or not isinstance(score, int):\n",
    "                continue\n",
    "            flat.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"source_id\": source_id,\n",
    "                \"rule_refs\": rule_refs,\n",
    "                \"reference_application\": ref_app,\n",
    "                \"student_answer\": student,\n",
    "                \"score\": score,\n",
    "            })\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb00230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== quiz_generate ===\n",
      "{'total': 30, 'parsed_ok': 30, 'parse_fail': 0, 'exact_match': 0, 'exact_match_rate': 0.0}\n",
      "samples:\n",
      "{'task': 'quiz_generate', 'gt': {'question': 'What whistle signal indicates intent to overtake on the port side in a narrow channel?', 'truth_answer': 'Two prolonged blasts followed by two short blasts.'}, 'pred': {'question': 'What signal indicates a vessel intends to overtake another on the port side?', 'truth_answer': 'Two prolonged blasts followed by two short blasts.'}}\n",
      "{'task': 'quiz_generate', 'gt': {'question': 'How are three vertical lights spaced?', 'truth_answer': 'Equally spaced.'}, 'pred': {'question': 'How far apart should three masthead lights be?', 'truth_answer': 'Equally spaced.'}}\n",
      "{'task': 'quiz_generate', 'gt': {'question': 'What defines a vessel constrained by her draught?', 'truth_answer': 'A power-driven vessel which because of her draught in relation to the available depth and width of navigable water is severely restricted in her ability to deviate from her course.'}, 'pred': {'question': 'What is a vessel constrained by her draught?', 'truth_answer': 'A power-driven vessel which because of her draught in relation to the available depth and width of navigable water is severely restricted in ability to deviate from the course she is following.'}}\n",
      "\n",
      "=== grade_truth_answer ===\n",
      "{'total': 40, 'parsed_ok': 40, 'parse_fail': 0, 'score_match': 34, 'score_match_rate': 0.85, 'mae_score_total': 0.225}\n",
      "samples:\n",
      "{'task': 'grade_truth_answer', 'gt_score_total': 0, 'pred_score_total': 0}\n",
      "{'task': 'grade_truth_answer', 'gt_score_total': 5, 'pred_score_total': 5}\n",
      "{'task': 'grade_truth_answer', 'gt_score_total': 10, 'pred_score_total': 10}\n",
      "\n",
      "=== rule_application_grade ===\n",
      "{'total': 120, 'parsed_ok': 120, 'parse_fail': 0, 'score_match': 71, 'score_match_rate': 0.5916666666666667, 'mae_score': 0.5916666666666667}\n",
      "samples:\n",
      "{'task': 'rule_application_grade', 'gt_score': 0, 'pred_score': 1}\n",
      "{'task': 'rule_application_grade', 'gt_score': 0, 'pred_score': 1}\n",
      "{'task': 'rule_application_grade', 'gt_score': 10, 'pred_score': 10}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation (adjust sample sizes if needed)\n",
    "quiz_report = evaluate_quiz_generate(n=30, seed=0)\n",
    "grade_report = evaluate_grade_truth_answer(n_questions=40, seed=0)\n",
    "scenario_report = evaluate_rule_application_grade(n=120, seed=0)\n",
    "\n",
    "print(\"=== quiz_generate ===\")\n",
    "print({k: v for k, v in quiz_report.items() if k != \"samples\"})\n",
    "print(\"samples:\")\n",
    "for s in quiz_report[\"samples\"]:\n",
    "    print(s)\n",
    "\n",
    "print(\"\\n=== grade_truth_answer ===\")\n",
    "print({k: v for k, v in grade_report.items() if k != \"samples\"})\n",
    "print(\"samples:\")\n",
    "for s in grade_report[\"samples\"]:\n",
    "    print(s)\n",
    "\n",
    "print(\"\\n=== rule_application_grade ===\")\n",
    "print({k: v for k, v in scenario_report.items() if k != \"samples\"})\n",
    "print(\"samples:\")\n",
    "for s in scenario_report[\"samples\"]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db8b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive quiz (4 questions) + grading ---\n",
    "# Uses the same TASK: prompts and SYSTEM_PROMPT_* as training/evaluation above.\n",
    "\n",
    "from typing import Optional\n",
    "import sys\n",
    "\n",
    "\n",
    "QUIZ4_TOPICS = [\n",
    "    \"Rule 5 (Look-out)\",\n",
    "    \"Rule 6 (Safe speed)\",\n",
    "    \"Rule 7 (Risk of collision)\",\n",
    "    \"Rule 8 (Action to avoid collision)\",\n",
    "    \"Rule 13 (Overtaking)\",\n",
    "    \"Rule 14 (Head-on situation)\",\n",
    "    \"Rule 15 (Crossing situation)\",\n",
    "    \"Rule 16 (Action by give-way vessel)\",\n",
    "    \"Rule 17 (Action by stand-on vessel)\",\n",
    "    \"Rule 19 (Conduct of vessels in restricted visibility)\",\n",
    " ]\n",
    "\n",
    "\n",
    "QUIZ_RUBRIC = [\n",
    "    {\n",
    "        \"id\": \"R1\",\n",
    "        \"max\": 5,\n",
    "        \"criterion\": \"Correctly identifies the relevant rule(s) and who must keep out of the way / stand on (if applicable).\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"R2\",\n",
    "        \"max\": 5,\n",
    "        \"criterion\": \"States the correct required actions/precautions (early & substantial action, avoid crossing ahead, safe speed, look-out, etc.) without adding incorrect claims.\",\n",
    "    },\n",
    " ]\n",
    "\n",
    "\n",
    "def _maybe_supporting_quotes(query: str, *, k: int = 6, max_chars: int = 420) -> list[dict]:\n",
    "    \"\"\"Return [{'ref','quote'}] from attached PDF if available; else [].\"\"\"\n",
    "    try:\n",
    "        if \"ATTACHED_PDF\" not in globals():\n",
    "            return []\n",
    "        pdf = globals().get(\"ATTACHED_PDF\")\n",
    "        if pdf is None:\n",
    "            return []\n",
    "        if \"retrieve_context_multiquery\" not in globals():\n",
    "            return []\n",
    "        retr = globals()[\"retrieve_context_multiquery\"]\n",
    "        indices = retr(query, k=min(int(k), 10))\n",
    "        chunks = getattr(pdf, \"chunks\", None)\n",
    "        if not isinstance(indices, list) or chunks is None:\n",
    "            return []\n",
    "        out: list[dict] = []\n",
    "        for idx in indices[:k]:\n",
    "            try:\n",
    "                ch = chunks[int(idx)]\n",
    "                txt = str(getattr(ch, \"text\", \"\") or \"\").strip()\n",
    "                if not txt:\n",
    "                    continue\n",
    "                out.append({\"ref\": f\"Chunk {int(idx)+1}\", \"quote\": txt[:max_chars]})\n",
    "            except Exception:\n",
    "                continue\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def _generate_one_quiz_item(*, topic: str, difficulty: str = \"\") -> dict[str, str]:\n",
    "    quotes = _maybe_supporting_quotes(topic)\n",
    "    user = (\n",
    "        \"TASK: quiz_generate\\n\"\n",
    "        + (f\"DIFFICULTY: {difficulty}\\n\" if difficulty else \"\")\n",
    "        + f\"RULE_REFS: {topic}\\n\"\n",
    "        + \"QUOTES:\\n\" + _format_quotes(quotes) + \"\\n\\n\"\n",
    "        + \"Generate ONE quiz item as JSON with keys: question, truth_answer.\"\n",
    "    )\n",
    "    raw, obj, err = _call_model_json(system_prompt=SYSTEM_PROMPT_QUIZ, user_prompt=user, max_tokens=256)\n",
    "    if obj is None:\n",
    "        raise RuntimeError(f\"Model did not return JSON: {err}. Raw: {raw[:200]}\")\n",
    "    q = str(obj.get(\"question\", \"\")).strip()\n",
    "    t = str(obj.get(\"truth_answer\", \"\")).strip()\n",
    "    if not q or not t:\n",
    "        raise RuntimeError(f\"Missing keys in model JSON. Got: {obj}\")\n",
    "    return {\"topic\": topic, \"question\": q, \"truth_answer\": t}\n",
    "\n",
    "\n",
    "def generate_quiz4(*, seed: int = 0) -> list[dict[str, str]]:\n",
    "    rng = random.Random(seed)\n",
    "    topics = QUIZ4_TOPICS[:]\n",
    "    rng.shuffle(topics)\n",
    "    picked = topics[:4]\n",
    "    difficulties = [\"easy\", \"medium\", \"medium\", \"hard\"]\n",
    "    quiz: list[dict[str, str]] = []\n",
    "    for i, topic in enumerate(picked):\n",
    "        diff = difficulties[i] if i < len(difficulties) else \"\"\n",
    "        # Retry once if the model fails to emit valid JSON\n",
    "        try:\n",
    "            quiz.append(_generate_one_quiz_item(topic=topic, difficulty=diff))\n",
    "        except Exception:\n",
    "            quiz.append(_generate_one_quiz_item(topic=topic, difficulty=diff))\n",
    "    return quiz\n",
    "\n",
    "\n",
    "def grade_user_answer(*, question: str, truth_answer: str, user_answer: str) -> dict[str, Any]:\n",
    "    support = _maybe_supporting_quotes(question + \"\\n\" + truth_answer, k=6)\n",
    "    r = {\n",
    "        \"question\": question,\n",
    "        \"truth_answer\": truth_answer,\n",
    "        \"rubric\": QUIZ_RUBRIC,\n",
    "        \"supporting_quotes\": support,\n",
    "    }\n",
    "    ua = {\"answer\": user_answer}\n",
    "    prompt = build_user_prompt_grade_truth(r, ua)\n",
    "    raw, obj, err = _call_model_json(system_prompt=SYSTEM_PROMPT_GRADE_TRUTH, user_prompt=prompt, max_tokens=512)\n",
    "    if obj is None:\n",
    "        raise RuntimeError(f\"Model did not return JSON: {err}. Raw: {raw[:300]}\")\n",
    "    # Normalize score_total\n",
    "    score_total = obj.get(\"score_total\")\n",
    "    if isinstance(score_total, bool):\n",
    "        score_total = int(score_total)\n",
    "    if isinstance(score_total, (int, float)) and float(score_total).is_integer():\n",
    "        score_total = int(score_total)\n",
    "    else:\n",
    "        score_total = None\n",
    "    if score_total is None:\n",
    "        raise RuntimeError(f\"Missing/invalid score_total in: {obj}\")\n",
    "    score_total = max(0, min(10, int(score_total)))\n",
    "    feedback = str(obj.get(\"feedback\", \"\")).strip()\n",
    "    return {\"score_total\": score_total, \"feedback\": feedback, \"raw\": obj}\n",
    "\n",
    "\n",
    "def run_quiz4(*, seed: int = 0) -> None:\n",
    "    if not globals().get(\"MODEL_NAME\"):\n",
    "        raise RuntimeError(\"MODEL_NAME not set\")\n",
    "    if not globals().get(\"bot\"):\n",
    "        raise RuntimeError(\"bot not initialized\")\n",
    "    if not _maybe_supporting_quotes(\"Rule 5\", k=1):\n",
    "        print(\"Note: No PDF context attached; QUOTES/SUPPORTING_QUOTES will be empty.\", flush=True)\n",
    "        print(\"For best grounding, run the PDF attachment section and attach the COLREG PDF first.\", flush=True)\n",
    "        print(\"\", flush=True)\n",
    "    quiz = generate_quiz4(seed=seed)\n",
    "    total = 0\n",
    "    print(\"--- COLREGS Quiz (4 questions) ---\", flush=True)\n",
    "    print(\"Type /exit to stop.\\n\", flush=True)\n",
    "    for i, item in enumerate(quiz, start=1):\n",
    "        q = item[\"question\"]\n",
    "        truth = item[\"truth_answer\"]\n",
    "        topic = item.get(\"topic\", \"\")\n",
    "        print(f\"Q{i} ({topic}): {q}\", flush=True)\n",
    "        try:\n",
    "            # Jupyter/VS Code can buffer prints; flush before waiting for input.\n",
    "            sys.stdout.flush()\n",
    "            ans = input(\"Your answer> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\n(quiz aborted)\")\n",
    "            return\n",
    "        if ans.lower().strip() in {\"/exit\", \"/quit\"}:\n",
    "            print(\"(quiz aborted)\")\n",
    "            return\n",
    "        try:\n",
    "            g = grade_user_answer(question=q, truth_answer=truth, user_answer=ans)\n",
    "            total += int(g.get(\"score_total\", 0) or 0)\n",
    "            print(f\"Score: {g['score_total']}/10\", flush=True)\n",
    "            if g.get(\"feedback\"):\n",
    "                print(\"Feedback:\", g[\"feedback\"], flush=True)\n",
    "        except Exception as e:\n",
    "            print(\"Grading failed:\", e, flush=True)\n",
    "        print(\"\", flush=True)\n",
    "    print(f\"Total: {total}/40\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc27753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COLREGS Quiz (4 questions) ---\n",
      "Type /exit to stop.\n",
      "\n",
      "Q1 (Rule 16 (Action by give-way vessel)): What must a give-way vessel do?\n",
      "Score: 5/10\n",
      "Feedback: Basic requirement correct but misses the 'early and substantial' aspect.\n",
      "\n",
      "Q2 (Rule 17 (Action by stand-on vessel)): What must a stand-on vessel do when the give-way vessel fails to act?\n",
      "Score: 0/10\n",
      "Feedback: Incorrect; Rule 17(a)(ii) allows stand-on vessel to take action alone if give-way vessel does not comply.\n",
      "\n",
      "Q3 (Rule 6 (Safe speed)): What must a vessel using a Traffic Separation Scheme do?\n",
      "Score: 10/10\n",
      "Feedback: Complete answer covering all requirements of Rule 10.\n",
      "\n",
      "Q4 (Rule 14 (Head-on situation)): What must each vessel do in a head-on situation?\n",
      "Score: 6/10\n",
      "Feedback: Basic concept is correct but lacks specificity about which vessel gives way and what course alteration is required.\n",
      "\n",
      "Total: 21/40\n"
     ]
    }
   ],
   "source": [
    "# Start the interactive quiz\n",
    "run_quiz4(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe8784",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42c5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\celli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\celli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PDF attachment (Hybrid RAG: BM25 + Embeddings + Multi-Query + CoVe) ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Install / import PDF extraction\n",
    "try:\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pypdf\"])\n",
    "    from pypdf import PdfReader  # type: ignore\n",
    "\n",
    "# Widgets (optional UI)\n",
    "try:\n",
    "    import ipywidgets as widgets  # type: ignore\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    display = None\n",
    "\n",
    "# Hybrid retrieval deps\n",
    "import numpy as np\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rank-bm25\"])\n",
    "    from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "except Exception:\n",
    "    SentenceTransformer = None  # type: ignore\n",
    "\n",
    "\n",
    "_WORD_RE = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "_STOPWORDS = {\n",
    "    # EN\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"is\",\"are\",\"was\",\"were\",\"be\",\"as\",\"at\",\"by\",\"it\",\"this\",\"that\",\"from\",\"into\",\"over\",\"under\",\"not\",\n",
    "    # FR\n",
    "    \"le\",\"la\",\"les\",\"un\",\"une\",\"des\",\"et\",\"ou\",\"de\",\"du\",\"dans\",\"sur\",\"pour\",\"avec\",\"est\",\"sont\",\"été\",\"être\",\"ce\",\"cet\",\"cette\",\"ces\",\"pas\",\"plus\",\n",
    "}\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    words = [w.lower() for w in _WORD_RE.findall(text)]\n",
    "    return [w for w in words if len(w) >= 2 and w not in _STOPWORDS]\n",
    "\n",
    "\n",
    "def _pdf_bytes_to_text(data: bytes) -> str:\n",
    "    reader = PdfReader(io.BytesIO(data))\n",
    "    parts: List[str] = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            parts.append(page.extract_text() or \"\")\n",
    "        except Exception:\n",
    "            parts.append(\"\")\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "\n",
    "def load_pdf_from_path(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return _pdf_bytes_to_text(f.read())\n",
    "\n",
    "\n",
    "_COLREG_HEAD_RE = re.compile(\n",
    "    r\"(?im)^(rule\\s+\\d+\\b.*|annex\\s+[ivx]+\\b.*|appendix\\b.*)$\"\n",
    " )\n",
    "\n",
    "\n",
    "def _normalize_ws(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def _slice_tokens(text: str, *, max_tokens: int, overlap_tokens: int) -> List[str]:\n",
    "    \"\"\"Approx token-based slicing (whitespace tokens).\"\"\"\n",
    "    text = _normalize_ws(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    toks = text.split()\n",
    "    if not toks:\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    start = 0\n",
    "    n = len(toks)\n",
    "    max_tokens = max(1, int(max_tokens))\n",
    "    overlap_tokens = max(0, int(overlap_tokens))\n",
    "    while start < n:\n",
    "        end = min(n, start + max_tokens)\n",
    "        out.append(\" \".join(toks[start:end]))\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = max(0, end - overlap_tokens)\n",
    "    return out\n",
    "\n",
    "\n",
    "def chunk_colreg(\n",
    "    text: str,\n",
    "    *,\n",
    "    chunk_tokens: int = 500,\n",
    "    overlap_tokens: int = 150,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Chunk COLREG-like docs by headings (Rule/Annex), then slice each section.\n",
    "\n",
    "    Requirement mapping:\n",
    "    - chunk size ~500 tokens\n",
    "    - overlap ~150 tokens\n",
    "    \"\"\"\n",
    "    raw = text.replace(\"\\r\\n\", \"\\n\")\n",
    "    lines = [ln.strip() for ln in raw.split(\"\\n\")]\n",
    "    headings: List[Tuple[int, str]] = []\n",
    "    for i, ln in enumerate(lines):\n",
    "        if not ln:\n",
    "            continue\n",
    "        m = _COLREG_HEAD_RE.match(ln)\n",
    "        if m:\n",
    "            headings.append((i, m.group(1).strip()))\n",
    "    if not headings:\n",
    "        joined = _normalize_ws(raw)\n",
    "        return [(\"Document\", ch) for ch in _slice_tokens(joined, max_tokens=chunk_tokens, overlap_tokens=overlap_tokens)]\n",
    "\n",
    "    chunks: List[Tuple[str, str]] = []\n",
    "    for idx, (start_i, title) in enumerate(headings):\n",
    "        end_i = headings[idx + 1][0] if idx + 1 < len(headings) else len(lines)\n",
    "        section_text = _normalize_ws(\" \".join([ln for ln in lines[start_i:end_i] if ln]))\n",
    "        if not section_text:\n",
    "            continue\n",
    "        for ch in _slice_tokens(section_text, max_tokens=chunk_tokens, overlap_tokens=overlap_tokens):\n",
    "            chunks.append((title, ch))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _minmax(scores: np.ndarray) -> np.ndarray:\n",
    "    if scores.size == 0:\n",
    "        return scores\n",
    "    mn = float(scores.min())\n",
    "    mx = float(scores.max())\n",
    "    if mx - mn < 1e-9:\n",
    "        return np.zeros_like(scores, dtype=float)\n",
    "    return (scores - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    idx: int\n",
    "    section: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttachedPDF:\n",
    "    name: str\n",
    "    text: str\n",
    "    chunks: List[Chunk]\n",
    "    bm25: BM25Okapi\n",
    "    bm25_tokens: List[List[str]]\n",
    "    embedder_name: str\n",
    "    embeddings: Optional[np.ndarray]  # shape: (n, d) or None\n",
    "\n",
    "\n",
    "ATTACHED_PDF: AttachedPDF | None = None\n",
    "RETRIEVE_K = 10\n",
    "BM25_WEIGHT = 0.55\n",
    "SEM_WEIGHT = 0.45\n",
    "EMBEDDER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_TOKENS = 1024\n",
    "CHUNK_OVERLAP_TOKENS = 256\n",
    "COVE_ENABLED = True\n",
    "_EMBEDDER: Optional[SentenceTransformer] = None  # type: ignore\n",
    "\n",
    "\n",
    "def _get_embedder() -> Optional[\"SentenceTransformer\"]:\n",
    "    global _EMBEDDER\n",
    "    if SentenceTransformer is None:\n",
    "        return None\n",
    "    if _EMBEDDER is None:\n",
    "        _EMBEDDER = SentenceTransformer(EMBEDDER_NAME)\n",
    "    return _EMBEDDER\n",
    "\n",
    "\n",
    "def attach_pdf_text(name: str, text: str) -> None:\n",
    "    \"\"\"Attach PDF text and build hybrid indexes (BM25 + embeddings).\"\"\"\n",
    "    global ATTACHED_PDF\n",
    "    pairs = chunk_colreg(text, chunk_tokens=CHUNK_TOKENS, overlap_tokens=CHUNK_OVERLAP_TOKENS)\n",
    "    chunks = [Chunk(idx=i, section=sec, text=ch) for i, (sec, ch) in enumerate(pairs)]\n",
    "    bm25_tokens = [_tokenize(c.text) for c in chunks]\n",
    "    bm25 = BM25Okapi(bm25_tokens)\n",
    "\n",
    "    embedder = _get_embedder()\n",
    "    embeddings: Optional[np.ndarray] = None\n",
    "    if embedder is not None:\n",
    "        docs = [f\"{c.section}. {c.text}\" for c in chunks]\n",
    "        emb = embedder.encode(docs, normalize_embeddings=True, show_progress_bar=False)\n",
    "        embeddings = np.asarray(emb, dtype=np.float32)\n",
    "\n",
    "    ATTACHED_PDF = AttachedPDF(\n",
    "        name=name,\n",
    "        text=text,\n",
    "        chunks=chunks,\n",
    "        bm25=bm25,\n",
    "        bm25_tokens=bm25_tokens,\n",
    "        embedder_name=EMBEDDER_NAME,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f\"Attached PDF: {name} ({len(text):,} chars, {len(chunks)} chunks)\")\n",
    "    print(f\"Chunking: ~{CHUNK_TOKENS} tokens, overlap ~{CHUNK_OVERLAP_TOKENS} tokens\")\n",
    "    if embeddings is None:\n",
    "        print(\"(Embeddings disabled: sentence-transformers not available)\")\n",
    "\n",
    "\n",
    "def clear_pdf() -> None:\n",
    "    global ATTACHED_PDF\n",
    "    ATTACHED_PDF = None\n",
    "    print(\"PDF cleared\")\n",
    "\n",
    "\n",
    "def _hybrid_scores(question: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert ATTACHED_PDF is not None\n",
    "    q_tokens = _tokenize(question)\n",
    "    bm25_scores = np.asarray(ATTACHED_PDF.bm25.get_scores(q_tokens), dtype=float)\n",
    "    bm25_norm = _minmax(bm25_scores)\n",
    "\n",
    "    sem_norm = np.zeros_like(bm25_norm, dtype=float)\n",
    "    if ATTACHED_PDF.embeddings is not None:\n",
    "        embedder = _get_embedder()\n",
    "        if embedder is not None:\n",
    "            q_emb = embedder.encode([question], normalize_embeddings=True, show_progress_bar=False)\n",
    "            q_emb = np.asarray(q_emb[0], dtype=np.float32)\n",
    "            sem = np.dot(ATTACHED_PDF.embeddings, q_emb).astype(float)\n",
    "            sem_norm = _minmax(sem)\n",
    "\n",
    "    hybrid = BM25_WEIGHT * bm25_norm + SEM_WEIGHT * sem_norm\n",
    "    return hybrid, bm25_norm, sem_norm\n",
    "\n",
    "\n",
    "def _diverse_topk(indices: List[int], *, k: int) -> List[int]:\n",
    "    \"\"\"Keep a diverse set of chunks by section to avoid near-duplicates.\"\"\"\n",
    "    assert ATTACHED_PDF is not None\n",
    "    picked: List[int] = []\n",
    "    seen_sections: Dict[str, int] = {}\n",
    "    for idx in indices:\n",
    "        sec = ATTACHED_PDF.chunks[idx].section\n",
    "        if seen_sections.get(sec, 0) >= 2:\n",
    "            continue\n",
    "        picked.append(idx)\n",
    "        seen_sections[sec] = seen_sections.get(sec, 0) + 1\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "    return picked\n",
    "\n",
    "\n",
    "def retrieve_context(question: str, *, k: int = 6, pool: int = 20) -> List[int]:\n",
    "    \"\"\"Hybrid retrieval with diversity: returns chunk indices.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        return []\n",
    "    hybrid, _, _ = _hybrid_scores(question)\n",
    "    ranked = np.argsort(-hybrid)[: max(pool, k)].tolist()\n",
    "    ranked = [i for i in ranked if hybrid[i] > 0]\n",
    "    return _diverse_topk(ranked, k=k)\n",
    "\n",
    "\n",
    "def _build_context(indices: List[int]) -> str:\n",
    "    \"\"\"Hierarchy-aware context formatting (grouped by section).\"\"\"\n",
    "    assert ATTACHED_PDF is not None\n",
    "    if not indices:\n",
    "        return \"\"\n",
    "    # Preserve retrieval order but group by section for readability/hierarchy clarity.\n",
    "    by_section: Dict[str, List[int]] = {}\n",
    "    section_order: List[str] = []\n",
    "    for idx in indices:\n",
    "        sec = ATTACHED_PDF.chunks[idx].section\n",
    "        if sec not in by_section:\n",
    "            by_section[sec] = []\n",
    "            section_order.append(sec)\n",
    "        by_section[sec].append(idx)\n",
    "\n",
    "    blocks: List[str] = []\n",
    "    for sec in section_order:\n",
    "        blocks.append(f\"=== {sec} ===\")\n",
    "        for idx in by_section[sec]:\n",
    "            c = ATTACHED_PDF.chunks[idx]\n",
    "            blocks.append(f\"(Chunk {idx+1}) {c.text}\")\n",
    "        blocks.append(\"\")\n",
    "    return \"\\n\".join(blocks).strip()\n",
    "\n",
    "\n",
    "def _generate_subqueries(question: str) -> List[str]:\n",
    "    \"\"\"Multi-Query Retrieval: have the LLM generate 3 diverse sub-queries.\"\"\"\n",
    "    prompt = (\n",
    "        \"Generate exactly 3 short, diverse search sub-queries to retrieve passages from the COLREG document. \"\n",
    "        \"Each sub-query should be a keyword-style query and may include Rule/Annex numbers if relevant.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Output ONLY the 3 queries, one per line\\n\"\n",
    "        \"- No explanations\\n\"\n",
    "        \"- Keep each query <= 12 words\\n\\n\"\n",
    "        f\"User question: {question}\"\n",
    "    )\n",
    "    raw = bot.reply(prompt, max_tokens=96, temperature=0.2)\n",
    "    lines = [ln.strip(\" -\\t\") for ln in raw.splitlines() if ln.strip()]\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    out: List[str] = []\n",
    "    for ln in lines:\n",
    "        key = ln.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(ln)\n",
    "        if len(out) >= 3:\n",
    "            break\n",
    "    return out if out else [question]\n",
    "\n",
    "\n",
    "def retrieve_context_multiquery(question: str, *, k: int = 6) -> List[int]:\n",
    "    \"\"\"Two-pass retrieval: initial retrieval + 3 LLM sub-queries -> merged scoring.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        return []\n",
    "    # Pass 1 (seed)\n",
    "    init = retrieve_context(question, k=max(4, k), pool=30)\n",
    "    # Pass 2 (multi-query)\n",
    "    queries = _generate_subqueries(question)\n",
    "    merged_scores = np.zeros(len(ATTACHED_PDF.chunks), dtype=float)\n",
    "    for q in [question] + queries:\n",
    "        hybrid, _, _ = _hybrid_scores(q)\n",
    "        merged_scores = np.maximum(merged_scores, hybrid)\n",
    "    ranked = np.argsort(-merged_scores)[: 40].tolist()\n",
    "    ranked = [i for i in ranked if merged_scores[i] > 0]\n",
    "    # Ensure we keep some of the seed results\n",
    "    for idx in init:\n",
    "        if idx in ranked:\n",
    "            ranked.remove(idx)\n",
    "        ranked.insert(0, idx)\n",
    "    return _diverse_topk(ranked, k=k)\n",
    "\n",
    "\n",
    "def _cove_answer(question: str, *, context: str) -> str:\n",
    "    \"\"\"Chain-of-Verification (CoVe) without revealing internal reasoning: returns final answer only.\"\"\"\n",
    "    # 1) Draft answer\n",
    "    draft_prompt = (\n",
    "        \"Answer the QUESTION using ONLY the EXCERPTS. \"\n",
    "        \"Cite evidence like (Chunk 12). If the excerpts do not contain the answer, say you don't know.\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"Output ONLY the draft answer.\"\n",
    "    )\n",
    "    draft = bot.reply(draft_prompt, max_tokens=512, temperature=0.2).strip()\n",
    "\n",
    "    # 2) Verification questions\n",
    "    ver_prompt = (\n",
    "        \"Create exactly 3 verification questions that would check whether the DRAFT is fully supported by the EXCERPTS. \"\n",
    "        \"Focus on potential missing conditions, exceptions, or definitions.\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"DRAFT:\\n{draft}\\n\\n\"\n",
    "        \"Output ONLY the 3 verification questions, one per line.\"\n",
    "    )\n",
    "    ver_raw = bot.reply(ver_prompt, max_tokens=128, temperature=0.2)\n",
    "    ver_qs = [ln.strip(\" -\\t\") for ln in ver_raw.splitlines() if ln.strip()]\n",
    "    ver_qs = ver_qs[:3] if ver_qs else []\n",
    "    ver_q_block = \"\\n\".join(f\"- {q}\" for q in ver_qs) if ver_qs else \"- (none)\"\n",
    "\n",
    "    # 3) Answer verification questions strictly from excerpts\n",
    "    ver_ans_prompt = (\n",
    "        \"Using ONLY the EXCERPTS, answer each verification question briefly. \"\n",
    "        \"If an answer is not supported by the EXCERPTS, write 'Not supported'.\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"VERIFICATION QUESTIONS:\\n{ver_q_block}\\n\\n\"\n",
    "        \"Output ONLY the answers, one per line in the same order.\"\n",
    "    )\n",
    "    ver_answers = bot.reply(ver_ans_prompt, max_tokens=256, temperature=0.2).strip()\n",
    "\n",
    "    # 4) Final answer (only)\n",
    "    final_prompt = (\n",
    "        \"You are evaluating a model. Produce the FINAL answer using ONLY the EXCERPTS. \"\n",
    "        \"If something in the DRAFT is not supported by the verification answers, remove or correct it. \"\n",
    "        \"Cite chunks like (Chunk 12). If the excerpts do not contain the answer, say you don't know.\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"DRAFT:\\n{draft}\\n\\n\"\n",
    "        f\"VERIFICATION ANSWERS:\\n{ver_answers}\\n\\n\"\n",
    "        \"Output ONLY the final answer.\"\n",
    "    )\n",
    "    return bot.reply(final_prompt, max_tokens=1024, temperature=0.2).strip()\n",
    "\n",
    "\n",
    "def ask_on_pdf(question: str, *, k: int | None = None, multiquery: bool = True) -> str:\n",
    "    if not ATTACHED_PDF:\n",
    "        return bot.reply(question)\n",
    "    k = RETRIEVE_K if k is None else k\n",
    "    indices = retrieve_context_multiquery(question, k=k) if multiquery else retrieve_context(question, k=k)\n",
    "    context = _build_context(indices)\n",
    "    if COVE_ENABLED:\n",
    "        return _cove_answer(question, context=context)\n",
    "    prompt = (\n",
    "        \"You are evaluating a model. Answer the user's question using ONLY the provided document excerpts. \"\n",
    "        \"Cite the chunks you used like (Chunk 12). If the excerpts do not contain the answer, say you don't know.\\n\\n\"\n",
    "        f\"Document: {ATTACHED_PDF.name}\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\"\n",
    "    )\n",
    "    return bot.reply(prompt, max_tokens=1024)\n",
    "\n",
    "\n",
    "def _extract_first_upload(value) -> tuple[str, bytes] | None:\n",
    "    \"\"\"Return (filename, content_bytes) for the first uploaded file.\"\"\"\n",
    "    if not value:\n",
    "        return None\n",
    "    if isinstance(value, dict):\n",
    "        first_key = next(iter(value.keys()))\n",
    "        first = value[first_key]\n",
    "        if isinstance(first, dict) and \"content\" in first:\n",
    "            name = first.get(\"name\") or first_key\n",
    "            return name, first[\"content\"]\n",
    "        if isinstance(first, (bytes, bytearray)):\n",
    "            return first_key, bytes(first)\n",
    "    if isinstance(value, (tuple, list)):\n",
    "        first = value[0]\n",
    "        if isinstance(first, dict) and \"content\" in first:\n",
    "            name = first.get(\"name\") or \"uploaded.pdf\"\n",
    "            return name, first[\"content\"]\n",
    "        name = getattr(first, \"name\", \"uploaded.pdf\")\n",
    "        content = getattr(first, \"content\", None)\n",
    "        if content is None:\n",
    "            content = getattr(first, \"data\", None)\n",
    "        if content is None and isinstance(first, (bytes, bytearray)):\n",
    "            content = bytes(first)\n",
    "        if isinstance(content, (bytes, bytearray)):\n",
    "            return name, bytes(content)\n",
    "    name = getattr(value, \"name\", \"uploaded.pdf\")\n",
    "    content = getattr(value, \"content\", None)\n",
    "    if isinstance(content, (bytes, bytearray)):\n",
    "        return name, bytes(content)\n",
    "    return None\n",
    "\n",
    "\n",
    "def show_pdf_uploader() -> None:\n",
    "    if widgets is None or display is None:\n",
    "        print(\"ipywidgets not available in this environment. Use attach_pdf_text(load_pdf_from_path(...))\")\n",
    "        return\n",
    "    uploader = widgets.FileUpload(accept=\".pdf\", multiple=False)\n",
    "    button = widgets.Button(description=\"Load PDF\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _on_click(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            item = _extract_first_upload(uploader.value)\n",
    "            if not item:\n",
    "                print(\"Select a PDF first\")\n",
    "                return\n",
    "            name, data = item\n",
    "            text = _pdf_bytes_to_text(data)\n",
    "            attach_pdf_text(name, text)\n",
    "\n",
    "    button.on_click(_on_click)\n",
    "    display(widgets.VBox([widgets.HTML(\"<b>Attach a PDF</b>\"), uploader, button, out]))\n",
    "\n",
    "\n",
    "# Run this to attach a PDF via UI (if supported):\n",
    "# show_pdf_uploader()\n",
    "\n",
    "# Or attach from a local path (Windows example):\n",
    "# attach_pdf_text(\"my.pdf\", load_pdf_from_path(r\"C:\\\\path\\\\to\\\\file.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f794fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b210c86494854b74a9e25d80703f43f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Attach a PDF</b>'), FileUpload(value=(), accept='.pdf', description='Upload'), B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pdf_uploader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98cb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "def _print(*args, **kwargs) -> None:\n",
    "    \"\"\"print(..., flush=True) by default (important for VS Code notebook input loops).\"\"\"\n",
    "    kwargs.setdefault(\"flush\", True)\n",
    "    print(*args, **kwargs)\n",
    "\n",
    "\n",
    "def _extract_json_array(text: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Best-effort: find and parse a JSON array from a model response.\"\"\"\n",
    "    s = (text or \"\").strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"Empty model output\")\n",
    "    # Fast path: exact JSON\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, list):\n",
    "            return obj  # type: ignore[return-value]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Slow path: extract the first [...] block\n",
    "    start = s.find(\"[\")\n",
    "    end = s.rfind(\"]\")\n",
    "    if start >= 0 and end > start:\n",
    "        candidate = s[start : end + 1]\n",
    "        obj = json.loads(candidate)\n",
    "        if isinstance(obj, list):\n",
    "            return obj  # type: ignore[return-value]\n",
    "    raise ValueError(\"Could not parse JSON array from model output\")\n",
    "\n",
    "\n",
    "def generate_colregs_quiz(*, n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate a short COLREGS quiz (JSON objects). Requires an attached PDF.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        raise RuntimeError(\"No PDF attached. Attach COLREGS PDF first.\")\n",
    "    bot.reset()\n",
    "    prompt = (\n",
    "        \"You are generating a short quiz to test COLREGS knowledge. \"\n",
    "        \"Generate EXACTLY 3 questions. Output ONLY valid JSON: a JSON array of 3 objects.\\n\\n\"\n",
    "        \"Each object must have:\\n\"\n",
    "        \"- question: string (ask about a specific rule/definition/situation)\\n\"\n",
    "        \"- rule_refs: array of strings (e.g., ['Rule 5','Rule 7'] or ['Annex IV'])\\n\"\n",
    "        \"- ideal_answer: string (1-3 sentences, no chain-of-thought)\\n\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- Every question must be clearly answerable from COLREG text (avoid trick questions)\\n\"\n",
    "        \"- Keep each question <= 35 words\\n\"\n",
    "        \"- Mix difficulty: 1 easy, 1 medium, 1 harder\\n\"\n",
    "        \"- Do NOT mention fatigue or tiredness\\n\"\n",
    "        \"- Do NOT add extra keys\\n\"\n",
    "    )\n",
    "    raw = bot.reply(prompt, max_tokens=600, temperature=0.4)\n",
    "    quiz = _extract_json_array(raw)\n",
    "    quiz = quiz[:n]\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for item in quiz:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        q = str(item.get(\"question\", \"\")).strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        rule_refs = item.get(\"rule_refs\", [])\n",
    "        if not isinstance(rule_refs, list):\n",
    "            rule_refs = []\n",
    "        rule_refs = [str(x).strip() for x in rule_refs if str(x).strip()]\n",
    "        ideal = str(item.get(\"ideal_answer\", \"\")).strip()\n",
    "        out.append({\"question\": q, \"rule_refs\": rule_refs, \"ideal_answer\": ideal})\n",
    "    if len(out) != n:\n",
    "        raise RuntimeError(f\"Quiz generation failed (got {len(out)} questions). Try again.\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _grade_one(*, question: str, user_answer: str, ideal_answer: str, rule_refs: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Grade a single answer using retrieved excerpts. Returns {score, verdict, feedback}.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        raise RuntimeError(\"No PDF attached\")\n",
    "    # Retrieve based on question and the ideal answer (helps anchor retrieval).\n",
    "    retrieval_query = question if not ideal_answer else f\"{question}\\nExpected: {ideal_answer}\"\n",
    "    indices = retrieve_context_multiquery(retrieval_query, k=min(8, RETRIEVE_K))\n",
    "    context = _build_context(indices)\n",
    "    refs = \", \".join(rule_refs) if rule_refs else \"(not specified)\"\n",
    "    prompt = (\n",
    "        \"You are grading a student's COLREGS answer using ONLY the EXCERPTS.\\n\"\n",
    "        \"Return ONLY valid JSON with keys: score, verdict, feedback.\\n\\n\"\n",
    "        \"Scoring rules:\\n\"\n",
    "        \"- score must be 0, 0.5, or 1\\n\"\n",
    "        \"- verdict must be one of: 'incorrect', 'partial', 'correct'\\n\"\n",
    "        \"- feedback must be 1-3 short sentences and may cite chunks like (Chunk 12)\\n\\n\"\n",
    "        f\"Target rule refs (hint only): {refs}\\n\\n\"\n",
    "        f\"EXCERPTS:\\n{context if context else '[No relevant excerpts found]'}\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"IDEAL ANSWER (short):\\n{ideal_answer}\\n\\n\"\n",
    "        f\"STUDENT ANSWER:\\n{user_answer}\\n\\n\"\n",
    "    )\n",
    "    raw = bot.reply(prompt, max_tokens=400, temperature=0.2)\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except Exception:\n",
    "        # best-effort extraction\n",
    "        start = (raw or \"\").find(\"{\")\n",
    "        end = (raw or \"\").rfind(\"}\")\n",
    "        obj = json.loads(raw[start : end + 1]) if (start >= 0 and end > start) else {}\n",
    "    score = obj.get(\"score\", 0)\n",
    "    verdict = obj.get(\"verdict\", \"incorrect\")\n",
    "    feedback = obj.get(\"feedback\", \"\")\n",
    "    if score not in (0, 0.5, 1):\n",
    "        score = 0\n",
    "    if verdict not in (\"incorrect\", \"partial\", \"correct\"):\n",
    "        verdict = \"incorrect\"\n",
    "    feedback = str(feedback).strip()\n",
    "    return {\"score\": score, \"verdict\": verdict, \"feedback\": feedback}\n",
    "\n",
    "\n",
    "def _fatigue_heuristic(total_score: float, *, max_score: float) -> str:\n",
    "    \"\"\"Non-medical heuristic: map performance to a fatigue hint.\"\"\"\n",
    "    # Keep it simple and deterministic (avoid the model going off-script).\n",
    "    if max_score <= 0:\n",
    "        return \"Insufficient data to infer anything.\"\n",
    "    pct = total_score / max_score\n",
    "    if pct >= 0.85:\n",
    "        return (\n",
    "            \"Your answers were strong and consistent. This suggests your COLREGS recall is currently solid. \"\n",
    "            \"If you still feel tired, trust your body—this quiz is not a medical test.\"\n",
    "        )\n",
    "    if pct >= 0.55:\n",
    "        return (\n",
    "            \"Your answers were mixed. That can happen from normal knowledge gaps or from fatigue. \"\n",
    "            \"If this is unusual for you, consider taking a short break and re-trying later.\"\n",
    "        )\n",
    "    return (\n",
    "        \"You struggled to answer consistently. That can be caused by fatigue, stress, or simply unfamiliarity. \"\n",
    "        \"If safety-critical, treat this as a signal to pause and recover rather than pushing on.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_colregs_quiz() -> None:\n",
    "    \"\"\"Interactive 3-question quiz, then a short conclusion.\"\"\"\n",
    "    if not ATTACHED_PDF:\n",
    "        _print(\"No PDF attached. Attach COLREGS PDF first (show_pdf_uploader() or attach_pdf_text(...)).\")\n",
    "        return\n",
    "    try:\n",
    "        quiz = generate_colregs_quiz(n=3)\n",
    "    except Exception as e:\n",
    "        _print(f\"Quiz generation error: {e}\")\n",
    "        return\n",
    "    _print(\"\\n--- COLREGS Quiz (3 questions) ---\")\n",
    "    _print(\"Answer in your own words. Type '/exit' to stop the quiz.\\n\")\n",
    "    total = 0.0\n",
    "    max_total = float(len(quiz))\n",
    "    for i, item in enumerate(quiz, start=1):\n",
    "        q = item[\"question\"]\n",
    "        refs = item.get(\"rule_refs\", [])\n",
    "        refs_txt = (\"; \".join(refs)) if refs else \"\"\n",
    "        _print(f\"Q{i}: {q}\" + (f\"  [{refs_txt}]\" if refs_txt else \"\"))\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "            user_answer = input(\"Your answer> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            _print(\"\\n(quiz aborted)\")\n",
    "            return\n",
    "        if user_answer.lower().strip() in {\"/exit\", \"/quit\"}:\n",
    "            _print(\"(quiz aborted)\")\n",
    "            return\n",
    "        try:\n",
    "            graded = _grade_one(\n",
    "                question=q,\n",
    "                user_answer=user_answer,\n",
    "                ideal_answer=item.get(\"ideal_answer\", \"\"),\n",
    "                rule_refs=refs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _print(f\"Grading error: {e}\")\n",
    "            graded = {\"score\": 0, \"verdict\": \"incorrect\", \"feedback\": \"Could not grade from excerpts.\"}\n",
    "        total += float(graded.get(\"score\", 0) or 0)\n",
    "        verdict = graded.get(\"verdict\", \"incorrect\")\n",
    "        feedback = graded.get(\"feedback\", \"\")\n",
    "        _print(f\"Result: {verdict} (score {graded.get('score', 0)}/1)\")\n",
    "        if feedback:\n",
    "            _print(f\"Feedback: {feedback}\")\n",
    "        _print(\"\")\n",
    "        time.sleep(0.2)\n",
    "    _print(\"--- Quiz complete ---\")\n",
    "    _print(f\"Total score: {total:.1f}/{max_total:.0f}\")\n",
    "    _print(\"Fatigue hint (non-medical):\")\n",
    "    _print(_fatigue_heuristic(total, max_score=max_total))\n",
    "    _print(\"\")\n",
    "\n",
    "\n",
    "def chat_loop():\n",
    "    \"\"\"Interactive loop.\n",
    "\n",
    "    Commands:\n",
    "      /reset       clear chat history\n",
    "      /exit        quit\n",
    "      /pdf         show PDF status\n",
    "      /clearpdf    detach current PDF\n",
    "      /k N         set retrieval chunks (current default shown in /pdf)\n",
    "      /quiz        run a 3-question COLREGS quiz + conclusion\n",
    "    \"\"\"\n",
    "    global RETRIEVE_K\n",
    "    _print(f\"Connected to {getattr(bot, 'model', 'model')}. Type '/reset' or '/exit'.\")\n",
    "    _print(\"Use '/pdf' to show PDF status after attaching a PDF.\")\n",
    "    while True:\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "            user = input(\"You> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            _print(\"\\nBye\")\n",
    "            return\n",
    "        if not user:\n",
    "            continue\n",
    "        low = user.lower().strip()\n",
    "        if low in {\"/exit\", \"/quit\"}:\n",
    "            _print(\"Bye\")\n",
    "            return\n",
    "        if low == \"/reset\":\n",
    "            bot.reset()\n",
    "            _print(\"(history cleared)\")\n",
    "            continue\n",
    "        if low == \"/pdf\":\n",
    "            if ATTACHED_PDF:\n",
    "                _print(\n",
    "                    f\"(pdf attached: {ATTACHED_PDF.name}, {len(ATTACHED_PDF.chunks)} chunks, \"\n",
    "                    f\"k={RETRIEVE_K}, chunk_tokens={CHUNK_TOKENS}, overlap={CHUNK_OVERLAP_TOKENS}, \"\n",
    "                    f\"bm25_w={BM25_WEIGHT}, sem_w={SEM_WEIGHT}, cove={COVE_ENABLED})\"\n",
    "                )\n",
    "            else:\n",
    "                _print(\"(no pdf attached) -> run show_pdf_uploader() or attach_pdf_text(...)\")\n",
    "            continue\n",
    "        if low == \"/clearpdf\":\n",
    "            clear_pdf()\n",
    "            continue\n",
    "        if low == \"/quiz\":\n",
    "            run_colregs_quiz()\n",
    "            continue\n",
    "        if low == \"/k\" or low.startswith(\"/k \"):\n",
    "            parts = low.split()\n",
    "            if len(parts) != 2:\n",
    "                _print(\"Usage: /k 10\")\n",
    "                continue\n",
    "            try:\n",
    "                RETRIEVE_K = max(1, int(parts[1]))\n",
    "                _print(f\"(k set to {RETRIEVE_K})\")\n",
    "            except Exception:\n",
    "                _print(\"Usage: /k 10\")\n",
    "            continue\n",
    "        try:\n",
    "            # Keep interactive chat responsive: avoid multiquery (extra LLM calls) by default here.\n",
    "            answer = ask_on_pdf(user, multiquery=False) if ATTACHED_PDF else bot.reply(user)\n",
    "        except Exception as e:\n",
    "            _print(f\"Error: {e}\")\n",
    "            continue\n",
    "        _print(f\"Bot> {answer}\\n\")\n",
    "\n",
    "\n",
    "# Run interactive chat in the notebook output\n",
    "# chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "146d9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to llama3.1:latest. Type '/reset' or '/exit'.\n",
      "Use '/pdf' to show PDF status after attaching a PDF.\n",
      "(pdf attached: COLREG-Consolidated-2018.pdf, 55 chunks, k=10, chunk_tokens=1024, overlap=256, bm25_w=0.55, sem_w=0.45, cove=True)\n",
      "\n",
      "--- COLREGS Quiz (3 questions) ---\n",
      "Answer in your own words. Type '/exit' to stop the quiz.\n",
      "\n",
      "Q1: What is the minimum speed a vessel must have when navigating in a narrow channel?  [Rule 9]\n",
      "Result: incorrect (score 0/1)\n",
      "\n",
      "Q2: When overtaking another power-driven vessel, what must you do to avoid a collision?  [Rule 15]\n",
      "Result: partial (score 0.5/1)\n",
      "Feedback: The student correctly identified the overtaking vessel's responsibility but failed to mention giving at least two short blasts on the horn as required by Rule 13(b) and Rule 34(c)(i).\n",
      "\n",
      "Q3: What is the responsibility of an overtaking vessel when encountering another vessel head-on?  [Rule 14]\n",
      "Result: incorrect (score 0/1)\n",
      "Feedback: The student incorrectly applied Rule 17(c) to a head-on situation, which does not apply. The correct rule for a head-on situation is Rule 14.\n",
      "\n",
      "--- Quiz complete ---\n",
      "Total score: 0.5/3\n",
      "Fatigue hint (non-medical):\n",
      "You struggled to answer consistently. That can be caused by fatigue, stress, or simply unfamiliarity. If safety-critical, treat this as a signal to pause and recover rather than pushing on.\n",
      "\n",
      "Bye\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c21283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Set évaluation RAG.csv\n",
      "PDF: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\COLREG-Consolidated-2018.pdf\n",
      "Loaded 12 questions\n",
      "Attached PDF: COLREG-Consolidated-2018.pdf (104,429 chars, 55 chunks)\n",
      "Chunking: ~1024 tokens, overlap ~256 tokens\n"
     ]
    }
   ],
   "source": [
    "# --- Batch evaluation from CSV (RAG) ---\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_existing_path(*candidates: str) -> Path:\n",
    "    for c in candidates:\n",
    "        p = Path(c)\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    raise FileNotFoundError(f\"None of these paths exist: {candidates}\")\n",
    "\n",
    "\n",
    "def _read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        # Common fallbacks for Windows CSV exports\n",
    "        for enc in (\"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "\n",
    "# Notebook usually runs with CWD = DeepSeek_test/. These files are at the workspace root.\n",
    "CSV_PATH = _find_existing_path(\n",
    "    \"Set évaluation RAG.csv\",\n",
    "    str(Path(\"..\") / \"Set évaluation RAG.csv\"),\n",
    "    str(Path(\"..\") / \"..\" / \"Set évaluation RAG.csv\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\Set évaluation RAG.csv\",\n",
    ")\n",
    "PDF_PATH = _find_existing_path(\n",
    "    \"COLREG-Consolidated-2018.pdf\",\n",
    "    str(Path(\"..\") / \"COLREG-Consolidated-2018.pdf\"),\n",
    "    str(Path(\"..\") / \"..\" / \"COLREG-Consolidated-2018.pdf\"),\n",
    "    r\"C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\COLREG-Consolidated-2018.pdf\",\n",
    ")\n",
    "\n",
    "print(\"CSV:\", CSV_PATH)\n",
    "print(\"PDF:\", PDF_PATH)\n",
    "\n",
    "# Load CSV and keep only the Question column for now\n",
    "df_questions = _read_csv_robust(CSV_PATH)\n",
    "if \"Question\" not in df_questions.columns:\n",
    "    raise KeyError(f\"CSV must have a 'Question' column. Found: {list(df_questions.columns)}\")\n",
    "questions = df_questions[\"Question\"].dropna().astype(str).tolist()\n",
    "print(f\"Loaded {len(questions)} questions\")\n",
    "\n",
    "# Attach the PDF (only if not already attached or if you want to force reload)\n",
    "if ATTACHED_PDF is None or (ATTACHED_PDF.name != PDF_PATH.name):\n",
    "    attach_pdf_text(PDF_PATH.name, load_pdf_from_path(str(PDF_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84070ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1 : 51.774319648742676s\n",
      "q2 : 61.1369526386261s\n",
      "q3 : 74.2618477344513s\n",
      "q4 : 65.50696015357971s\n",
      "q5 : 63.7942955493927s\n",
      "q6 : 95.42311549186707s\n",
      "q7 : 92.42543840408325s\n",
      "q8 : 55.08546566963196s\n",
      "q9 : 109.19661474227905s\n",
      "q10 : 93.6745240688324s\n",
      "Done 10/12\n",
      "q11 : 99.64414310455322s\n",
      "q12 : 98.1123435497284s\n",
      "Done 12/12\n",
      "Saved: C:\\Users\\celli\\Documents\\.PIP2026\\PIP-2026-LOTUSim_G5_Fatigue\\DeepSeek_test\\rag_eval_answers.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Run the evaluation: ask each question on the attached PDF\n",
    "MAX_QUESTIONS = None  # set to an int for a quick dry-run, e.g. 5\n",
    "\n",
    "questions_to_run = questions if not MAX_QUESTIONS else questions[:MAX_QUESTIONS]\n",
    "results = []\n",
    "total = len(questions_to_run)\n",
    "for i, q in enumerate(questions_to_run, start=1):\n",
    "    bot.reset()  # avoid history leakage across questions\n",
    "    strict_prompt = \"\"\n",
    "    t0 = time.time()\n",
    "    ans = ask_on_pdf(strict_prompt + q)\n",
    "    print(f\"q{i} : {time.time() - t0}s\")\n",
    "    results.append({\"Question\": q, \"Answer\": ans})\n",
    "    if i % 10 == 0 or i == total:\n",
    "        print(f\"Done {i}/{total}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.head()\n",
    "\n",
    "# Save results next to the notebook\n",
    "OUT_PATH = Path(\"rag_eval_answers.csv\").resolve()\n",
    "df_results.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
